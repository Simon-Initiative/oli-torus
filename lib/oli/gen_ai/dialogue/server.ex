defmodule Oli.GenAI.Dialogue.Server do
  use GenServer
  require Logger

  @moduledoc """
  This module implements a dialogue server that manages the interaction
  between a user and a language model provider using the GenServer behavior.

  The dialogue is stateful and allows for asynchronous message handling,
  including function calls and streaming responses. It is designed to
  handle user messages, process them through the LLM, and manage the
  dialogue state effectively.

  The server is started with a static configuration that includes
  the service config to use, existing messages, functions available for calls,
  and a reply-to PID for sending responses back to the client.

  The server supports engaging in a dialogue by sending messages,
  processing responses, and automatically handling function calls. It can also
  handle streaming responses from the LLM, allowing for real-time
  interaction with the user.

  The dialogue server is designed to be resilient, with the ability to
  retry operations using a backup model if the primary model fails.
  It maintains a state that includes the current messages, configuration,
  and any pending function calls.

  From a client, as an example of how to use this server:

  ```elixir
  {:ok, server} = Oli.GenAI.Dialogue.Server.new(configuration)

  Oli.GenAI.Dialogue.Server.engage(server, %Oli.GenAI.Completions.Message{
    role: :user,
    content: "What is the capital of France?"
  })
  ```

  Then, the server will stream back tokens as they are generated by the LLM,
  and you can handle them in your client (probably a LiveViewO by listening to the
  server's messages, like so:

  ```elixir
  def handle_info({:tokens_received, content}, socket) do
    {:noreply, assign(socket, :active_message, socket.assigns.active_message <> content)}
  end

  def handle_info({:tokens_finished}, socket) do
    {:noreply, assign(socket, :streaming, false)}
  end
  ```

  """

  alias Oli.GenAI.Completions
  alias Oli.GenAI.Completions.{Message, Function}
  alias Oli.GenAI.Dialogue.{State, Configuration}

  @doc """
  Starts a dialogue server given a static configuration.
  """
  def new(%Configuration{} = static_configuration) do
    GenServer.start_link(__MODULE__, static_configuration)
  end

  def engage(server, %Message{} = message) when is_pid(server) do
    GenServer.cast(server, {:engage, message})
  end

  def init(%Configuration{} = static_configuration) do
    {:ok, State.new(static_configuration)}
  end

  def handle_cast({:engage, message}, %State{} = state) do

    # Extract the current dialogue state and configuration
    %{messages: messages} = state

    # spawn a Task so we don’t block the GenServer loop
    server = self()
    Task.start(fn -> do_engage(server, state, message) end)

    {:noreply, %State{state | messages: [message | messages]}}

  end

  defp do_engage(server_pid, %State{configuration: configuration, registered_model: registered_model} = state, %Message{} = message) do

    process_chunk = fn chunk ->
      case chunk do
        {:error} ->
          send_to_server(server_pid, {:stream_chunk, {:error}})
          send_to_listener(state, {:error})

        {:function_call, function_call} ->
          send_to_server(server_pid, {:stream_chunk, {:function_call, function_call}})

        {:function_call_finished} ->
          send_to_server(server_pid, {:stream_chunk, {:function_call_finished}})

        {:tokens_received, content} ->
          send_to_server(server_pid, {:stream_chunk, {:tokens_received, content}})
          send_to_listener(state, {:tokens_received, content})

        {:tokens_finished} ->
          send_to_server(server_pid, {:stream_chunk, {:tokens_finished}})
          send_to_listener(state, {:tokens_finished})
      end
    end

    response_handler_fn = fn response ->
      case response do
        chunks when is_list(chunks) ->
          Enum.each(chunks, fn chunk -> process_chunk.(chunk) end)

        chunk ->
          process_chunk.(chunk)
      end
    end

    case Completions.stream(state.messages ++ [message], configuration.functions, registered_model, response_handler_fn) do

      valid_results when is_list(valid_results) ->
        {:noreply, state}

      {:error, error} ->

        if should_retry?(state) do
          state = fallback(state)
          do_engage(server_pid, state, message)
        else
          {:error, error}
        end

    end
  end

  defp should_retry?(state) do
    state.configuration.service_config.backup_model &&
      state.configuration.service_config.backup_model != state.registered_model
  end

  defp fallback(state), do: Map.put(state, :registered_model, state.configuration.service_config.backup_model)

  defp send_to_listener(%State{configuration: %Configuration{reply_to_pid: pid}}, message) do
    send(pid, message)
  end

  defp send_to_server(server, message) do
    send(server, message)
  end

  # All stream chunks come back here

  def handle_info({:stream_chunk, {:tokens_received, content}}, state) do
    # Append tokens to the “assistant” draft
    messages = append_token(state.messages, content)
    {:noreply, %{state | messages: messages}}
  end

  def handle_info({:stream_chunk, {:function_call, content}}, state) do

    # we are processing a chunk of information regarding a function to call
    case content do
      %{"name" => name, "arguments" => args} ->
        {:noreply, %{state | pending_function_args: args, pending_function_name: name}}

      %{"arguments" => args} ->
        {:noreply, %{state | pending_function_args: state.pending_function_args <> args}}
    end

  end

  def handle_info({:stream_chunk, {:function_call_finished}}, state) do

    # The LLM has finished spitting out the function args, we need to execute it.
    %{pending_function_name: name, pending_function_args: args, configuration: configuration} = state
    %{functions: functions} = configuration

    case Jason.decode(args) do
      {:ok, arguments_as_map} ->
        case Function.call(functions, name, arguments_as_map) do
          {:ok, result} ->

            message = %Message{
              role: :function,
              content: result,
              name: name
            }

            Logger.info("Function #{name} executed successfully with result: #{result}")

            state = %{state | pending_function_args: nil, pending_function_name: nil, pending_function_message: message}
            {:noreply, state, {:continue, :execute_function}}

          {:error, reason} ->
            Logger.error("Failed to execute function #{name}: #{reason}")
            {:noreply, state}
        end

      {:error, _} ->
        Logger.error("Failed to decode function arguments: #{args}")
        {:noreply, state}
    end

  end

  def handle_info({:stream_chunk, {:tokens_finished}}, state) do
    # done streaming tokens; nothing more to do here
    {:noreply, state}
  end

  def handle_continue(:execute_function, %{pending_function_message: pending_message} = state) do
    # We have executed the function successfully, now we want to
    # engage the dialogue with the result, by posting a message
    send(self(), {:engage, pending_message})
    {:noreply, state}
  end

  # This is the entry point for engaging the dialogue server from within the server itself.
  def handle_info({:engage, message}, %State{} = state) do

    %{messages: messages} = state

    server = self()
    Task.start(fn -> do_engage(server, state, message) end)

    {:noreply, %State{state | messages: [message | messages]}}

  end

  defp append_token(messages, token) do

    case Enum.reverse(messages) do

      [%{role: :assistant, content: old} = last | rest] ->
        updated = %{last | content: old <> token}
        Enum.reverse([updated | rest])

      list ->
        # If the last message is not from the assistant, we need to create a new one
        new_message = Message.new(:assistant, token)
        Enum.reverse([new_message | list])

    end
  end

end
