{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21b93cb",
   "metadata": {},
   "source": [
    "# OLI Torus XAPI ETL Pipeline - XAPI Events Processing\n",
    "\n",
    "This notebook provides an interface for bulk processing of historical XAPI data using AWS Lambda.\n",
    "\n",
    "### Supported Event Types:\n",
    "- `video` - Video interactions (play, pause, seek, complete)\n",
    "- `activity_attempt` - Activity completion events  \n",
    "- `page_attempt` - Page completion events\n",
    "- `page_viewed` - Page view tracking\n",
    "- `part_attempt` - Individual question responses\n",
    "\n",
    "## S3 ETL Performance\n",
    "\n",
    "The ETL pipeline uses **pure ClickHouse S3 ingestion** for all operations:\n",
    "\n",
    "- **ClickHouse S3 Ingestion**: Direct processing using ClickHouse's native `s3()` table functions\n",
    "- **Unified Processing**: Single high-performance SQL query for all event types\n",
    "- **No Timeout Limits**: Bypasses Lambda's 15-minute limitation completely\n",
    "- **Consistent Performance**: Same fast processing for any dataset size\n",
    "- **Cost Effective**: Minimal Lambda execution time - ClickHouse does the heavy lifting\n",
    "\n",
    "## Performance Benefits\n",
    "\n",
    "- **10x Faster Processing**: ClickHouse S3 integration + unified table provides significant performance improvements\n",
    "- **No Lambda Timeout Limits**: Process unlimited datasets without time constraints\n",
    "- **Simplified Queries**: Single table queries are much faster than multi-table JOINs\n",
    "- **Better Indexing**: LowCardinality event_type field enables efficient filtering and grouping\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **AWS Configuration**: AWS credentials with Lambda invoke and S3 access permissions\n",
    "2. **ClickHouse Access**: ClickHouse instance with S3 access and `raw_events` table created\n",
    "3. **Lambda Deployment**: Updated Lambda functions deployed with unified table support\n",
    "4. **Environment Setup**: Proper .env configuration for AWS and ClickHouse credentials\n",
    "\n",
    "## Compatibility\n",
    "\n",
    "- ‚úÖ **Google Colab**: All dependencies installable via pip, no local file dependencies\n",
    "- ‚úÖ **Local Jupyter**: Full functionality including direct local execution mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7bc98",
   "metadata": {},
   "source": [
    "## Quick Setup - Install Requirements\n",
    "\n",
    "Run the cell below to install all required dependencies for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc07367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required dependencies\n",
    "# Choose one of the options below:\n",
    "\n",
    "# Option 1: Install notebook-specific requirements (includes pandas and visualization tools)\n",
    "!pip install -r notebook-requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae94f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Environment Variables using python-dotenv\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Install python-dotenv if not already installed\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"üîß Loading environment variables from .env file...\")\n",
    "\n",
    "# Load .env file - this is much simpler than manual parsing!\n",
    "if load_dotenv('.env'):\n",
    "    print(\"‚úÖ Environment variables loaded successfully!\")\n",
    "\n",
    "    # Check AWS configuration\n",
    "    AWS_REGION = os.getenv('AWS_REGION', 'us-east-1')\n",
    "    AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "    AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "    S3_XAPI_BUCKET = os.getenv('S3_XAPI_BUCKET')\n",
    "\n",
    "    print(f\"\\nüîç AWS Configuration Status:\")\n",
    "    print(f\"  AWS_ACCESS_KEY_ID: {'‚úÖ Set' if AWS_ACCESS_KEY_ID else '‚ùå Missing'}\")\n",
    "    print(f\"  AWS_SECRET_ACCESS_KEY: {'‚úÖ Set' if AWS_SECRET_ACCESS_KEY else '‚ùå Missing'}\")\n",
    "    print(f\"  AWS_REGION: {AWS_REGION if AWS_REGION else '‚ùå Missing'}\")\n",
    "    print(f\"  S3_XAPI_BUCKET: {S3_XAPI_BUCKET if S3_XAPI_BUCKET else '‚ùå Missing'}\")\n",
    "\n",
    "    # Initialize AWS clients\n",
    "    lambda_client = boto3.client('lambda', region_name=AWS_REGION)\n",
    "    s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "    ENVIRONMENT = os.getenv('ENVIRONMENT', 'dev')\n",
    "\n",
    "    # Unified Lambda function name (adjust based on your deployment)\n",
    "    XAPI_ETL_FUNCTION = f'xapi-etl-processor-{ENVIRONMENT}'\n",
    "\n",
    "    print(f\"Configured for {ENVIRONMENT} environment in {AWS_REGION}\")\n",
    "    print(f\"Unified XAPI ETL Function: {XAPI_ETL_FUNCTION}\")\n",
    "\n",
    "    EXECUTION_MODE = os.getenv('EXECUTION_MODE', 'local').lower()\n",
    "\n",
    "    # For local execution, we'll import the required modules\n",
    "    if EXECUTION_MODE == 'local':\n",
    "        import sys\n",
    "        import os\n",
    "\n",
    "        # Add current directory to path to import local modules\n",
    "        current_dir = os.path.dirname(os.path.abspath('__file__' if '__file__' in globals() else os.getcwd()))\n",
    "        if current_dir not in sys.path:\n",
    "            sys.path.insert(0, current_dir)\n",
    "\n",
    "        # Import local modules (using unified versions)\n",
    "        try:\n",
    "            from lambda_function import lambda_handler, health_check\n",
    "            from common import get_config\n",
    "            from clickhouse_client import ClickHouseClient\n",
    "            print(f\"‚úÖ Local unified modules imported successfully\")\n",
    "            print(f\"   - Using unified raw_events table model\")\n",
    "            print(f\"   - All event types processed into single table\")\n",
    "        except ImportError as e:\n",
    "            print(f\"‚ùå Failed to import local modules: {e}\")\n",
    "            print(\"Make sure you're running this notebook from the xapi-etl-processor directory\")\n",
    "            print(\"Exiting due to failed local module imports.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    print(f\"üîß Execution mode: {EXECUTION_MODE.upper()}\")\n",
    "    if EXECUTION_MODE == 'lambda':\n",
    "        print(f\"   Using Lambda function: {XAPI_ETL_FUNCTION}\")\n",
    "        print(f\"   üìä Lambda processes via unified raw_events table\")\n",
    "    else:\n",
    "        print(f\"   Running locally with unified table model\")\n",
    "        print(f\"   üìä Direct ClickHouse processing into raw_events table\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Could not load .env file\")\n",
    "    print(f\"   Make sure .env file exists with your AWS credentials:\")\n",
    "    print(f\"   cp example.env .env\")\n",
    "    print(f\"   # Then edit .env with your actual credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b06a0",
   "metadata": {},
   "source": [
    "## Execution Modes\n",
    "\n",
    "This notebook supports two execution modes:\n",
    "\n",
    "### Local Mode (Default)\n",
    "- **EXECUTION_MODE = 'local'**\n",
    "- Runs the Lambda function code locally in this notebook\n",
    "- Requires local modules (lambda_function.py, common.py, clickhouse_client.py)\n",
    "- Good for testing and development\n",
    "- All operations run synchronously in the notebook\n",
    "\n",
    "### Lambda Mode\n",
    "- **EXECUTION_MODE = 'lambda'**\n",
    "- Invokes the deployed AWS Lambda function remotely\n",
    "- Requires AWS credentials and deployed Lambda function\n",
    "- Best for production use and processing large datasets\n",
    "- Supports true asynchronous processing\n",
    "\n",
    "**To switch modes:** Change the `EXECUTION_MODE` variable in the cell above and re-run that cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265f6a9",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae00b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_function(payload):\n",
    "    \"\"\"Execute function either locally or via Lambda based on EXECUTION_MODE\"\"\"\n",
    "    if EXECUTION_MODE == 'local':\n",
    "        return execute_local(payload)\n",
    "    else:\n",
    "        return invoke_lambda_sync(payload)\n",
    "\n",
    "def execute_function_async(payload):\n",
    "    \"\"\"Execute function asynchronously - Lambda only or simulate locally\"\"\"\n",
    "    if EXECUTION_MODE == 'local':\n",
    "        # For local execution, run synchronously but indicate it's \"async\"\n",
    "        result = execute_local(payload)\n",
    "        if result['success']:\n",
    "            result['async_mode'] = True\n",
    "            result['request_id'] = f\"local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "        return result\n",
    "    else:\n",
    "        return invoke_lambda_async(payload)\n",
    "\n",
    "def execute_local(payload):\n",
    "    \"\"\"Execute the Lambda function locally\"\"\"\n",
    "    try:\n",
    "        # Create a mock context object\n",
    "        class MockContext:\n",
    "            def __init__(self):\n",
    "                self.function_name = \"xapi-etl-processor-local\"\n",
    "                self.function_version = \"1\"\n",
    "                self.aws_request_id = f\"local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "        context = MockContext()\n",
    "\n",
    "        # Call the lambda_handler function directly\n",
    "        result = lambda_handler(payload, context)\n",
    "\n",
    "        return {\n",
    "            'success': True,\n",
    "            'status_code': result.get('statusCode', 200),\n",
    "            'result': result,\n",
    "            'execution_mode': 'local'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'execution_mode': 'local'\n",
    "        }\n",
    "\n",
    "def invoke_lambda_async(payload):\n",
    "    \"\"\"Invoke the unified Lambda function asynchronously\"\"\"\n",
    "    try:\n",
    "        response = lambda_client.invoke(\n",
    "            FunctionName=XAPI_ETL_FUNCTION,\n",
    "            InvocationType='Event',  # Async invocation\n",
    "            Payload=json.dumps(payload)\n",
    "        )\n",
    "        return {\n",
    "            'success': True,\n",
    "            'status_code': response['StatusCode'],\n",
    "            'request_id': response['ResponseMetadata']['RequestId'],\n",
    "            'execution_mode': 'lambda'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'execution_mode': 'lambda'\n",
    "        }\n",
    "\n",
    "def invoke_lambda_sync(payload):\n",
    "    \"\"\"Invoke the unified Lambda function synchronously\"\"\"\n",
    "    try:\n",
    "        response = lambda_client.invoke(\n",
    "            FunctionName=XAPI_ETL_FUNCTION,\n",
    "            InvocationType='RequestResponse',  # Sync invocation\n",
    "            Payload=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "        result = json.loads(response['Payload'].read().decode())\n",
    "        return {\n",
    "            'success': True,\n",
    "            'status_code': response['StatusCode'],\n",
    "            'result': result,\n",
    "            'execution_mode': 'lambda'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'execution_mode': 'lambda'\n",
    "        }\n",
    "\n",
    "def check_function_health():\n",
    "    \"\"\"Check if the function is healthy (Lambda or local)\"\"\"\n",
    "    payload = {'health_check': True}\n",
    "    return execute_function(payload)\n",
    "\n",
    "print(\"Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd18091",
   "metadata": {},
   "source": [
    "## 1. Health Checks\n",
    "\n",
    "First, let's verify that our Lambda functions and ClickHouse are healthy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7014ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check XAPI ETL processor health\n",
    "print(\"Checking XAPI ETL processor health...\")\n",
    "health = check_function_health()\n",
    "print(json.dumps(health, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40619cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clear existing section data before reprocessing\n",
    "# Uncomment and run this cell if you want to start fresh for this section\n",
    "\n",
    "clear_section_data = True  # Set to True to enable clearing\n",
    "target_section_id = '145'   # Make sure this matches your section ID above\n",
    "\n",
    "if clear_section_data:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: This will delete ALL existing data for section {target_section_id}\")\n",
    "    print(f\"   üóÉÔ∏è  From unified raw_events table (all event types)\")\n",
    "\n",
    "    if EXECUTION_MODE == 'local':\n",
    "        try:\n",
    "            # For local execution, directly use ClickHouse client\n",
    "            from clickhouse_client import ClickHouseClient\n",
    "            clickhouse_client = ClickHouseClient()\n",
    "\n",
    "            # Delete all events for this section from the unified table\n",
    "            print(f\"üßπ Clearing unified raw_events table for section {target_section_id}...\")\n",
    "\n",
    "            try:\n",
    "                # Get current count before deletion\n",
    "                current_count = clickhouse_client.get_section_event_count(int(target_section_id))\n",
    "                print(f\"   üìä Current events in section: {current_count}\")\n",
    "\n",
    "                if current_count > 0:\n",
    "                    # Delete all events for this section\n",
    "                    result = clickhouse_client.delete_section_events(int(target_section_id))\n",
    "                    print(f\"‚úÖ Cleared all events for section {target_section_id} from raw_events table\")\n",
    "\n",
    "                    # Verify deletion\n",
    "                    remaining_count = clickhouse_client.get_section_event_count(int(target_section_id))\n",
    "                    print(f\"   üìä Remaining events: {remaining_count}\")\n",
    "                else:\n",
    "                    print(f\"   üí° No existing events found for section {target_section_id}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error clearing section data: {str(e)}\")\n",
    "\n",
    "            print(f\"‚úÖ Section {target_section_id} data cleared - you can now set force_reprocess=False\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to clear section data: {str(e)}\")\n",
    "    else:\n",
    "        print(\"‚ùå Data clearing only available in local mode\")\n",
    "        print(\"   For Lambda mode, use force_reprocess=True instead\")\n",
    "        print(\"   üí° The unified table approach makes clearing more efficient\")\n",
    "else:\n",
    "    print(\"üí° Section data clearing disabled\")\n",
    "    print(\"   Set clear_section_data = True above to enable section data clearing\")\n",
    "    print(\"   üóÉÔ∏è  Unified table: All event types stored in single raw_events table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de0658",
   "metadata": {},
   "source": [
    "## 4. Bulk Process Multiple Sections\n",
    "\n",
    "Process data for multiple sections (useful for large-scale historical data loading):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple specific sections\n",
    "sections_to_process = [145]  # Add your section IDs here\n",
    "batch_size = 5  # Process sections in batches to avoid timeouts\n",
    "force_reprocess_batch = False  # Set to True to reprocess existing data\n",
    "\n",
    "print(f\"üîÑ Processing {len(sections_to_process)} sections with unified approach\")\n",
    "print(f\"üì¶ Batch size: {batch_size} sections per batch\")\n",
    "print(f\"üóÉÔ∏è  Target: Unified raw_events table\")\n",
    "\n",
    "if sections_to_process:\n",
    "    successful_sections = []\n",
    "    failed_sections = []\n",
    "\n",
    "    # Process sections in batches\n",
    "    for i in range(0, len(sections_to_process), batch_size):\n",
    "        batch = sections_to_process[i:i+batch_size]\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        total_batches = (len(sections_to_process) + batch_size - 1) // batch_size\n",
    "\n",
    "        print(f\"\\nüì¶ Processing batch {batch_num}/{total_batches}: {batch}\")\n",
    "\n",
    "        for section_id in batch:\n",
    "            try:\n",
    "                # Create payload for this section\n",
    "                section_payload = {\n",
    "                    'mode': 'single_section',\n",
    "                    'section_id': section_id,\n",
    "                    'force_reprocess': force_reprocess_batch,\n",
    "                    's3_bucket': S3_XAPI_BUCKET,\n",
    "                    's3_prefix': f'section/{section_id}/'\n",
    "                }\n",
    "\n",
    "                print(f\"  üéØ Processing section {section_id}...\")\n",
    "\n",
    "                if EXECUTION_MODE == 'local':\n",
    "                    # For local execution, use the unified ClickHouse client\n",
    "                    from clickhouse_client import ClickHouseClient\n",
    "                    clickhouse_client = ClickHouseClient()\n",
    "\n",
    "                    # Build S3 paths for this section - we need to list files first\n",
    "                    try:\n",
    "                        # List all files for this section\n",
    "                        s3_prefix = f'section/{section_id}/'\n",
    "                        response = s3_client.list_objects_v2(\n",
    "                            Bucket=S3_XAPI_BUCKET,\n",
    "                            Prefix=s3_prefix\n",
    "                        )\n",
    "\n",
    "                        if 'Contents' in response:\n",
    "                            # Build S3 paths for bulk processing\n",
    "                            s3_paths = []\n",
    "                            for obj in response['Contents']:\n",
    "                                if obj['Key'].endswith('.jsonl'):\n",
    "                                    s3_path = f\"s3://{S3_XAPI_BUCKET}/{obj['Key']}\"\n",
    "                                    s3_paths.append(s3_path)\n",
    "\n",
    "                            if s3_paths:\n",
    "                                print(f\"    üìÅ Found {len(s3_paths)} JSONL files for section {section_id}\")\n",
    "\n",
    "                                # Use bulk_insert_from_s3 method (this is the correct method)\n",
    "                                result = clickhouse_client.bulk_insert_from_s3(\n",
    "                                    s3_paths=s3_paths,\n",
    "                                    section_id=section_id\n",
    "                                )\n",
    "\n",
    "                                print(f\"    ‚úÖ Local processing completed: {result['total_events_processed']} events processed\")\n",
    "                                successful_sections.append(section_id)\n",
    "                            else:\n",
    "                                print(f\"    ‚ö†Ô∏è  No JSONL files found for section {section_id}\")\n",
    "                                failed_sections.append(section_id)\n",
    "                        else:\n",
    "                            print(f\"    ‚ö†Ô∏è  No objects found with prefix {s3_prefix}\")\n",
    "                            failed_sections.append(section_id)\n",
    "\n",
    "                    except Exception as s3_error:\n",
    "                        print(f\"    ‚ùå Error listing S3 files for section {section_id}: {str(s3_error)}\")\n",
    "                        failed_sections.append(section_id)\n",
    "\n",
    "                else:\n",
    "                    # For Lambda execution\n",
    "                    result = execute_function_async(section_payload)\n",
    "\n",
    "                    if result['success']:\n",
    "                        print(f\"    ‚úÖ Lambda triggered for section {section_id}\")\n",
    "                        print(f\"       Request ID: {result['request_id']}\")\n",
    "                        successful_sections.append(section_id)\n",
    "                    else:\n",
    "                        print(f\"    ‚ùå Failed to process section {section_id}: {result.get('error')}\")\n",
    "                        failed_sections.append(section_id)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Error processing section {section_id}: {str(e)}\")\n",
    "                failed_sections.append(section_id)\n",
    "\n",
    "        # Small delay between batches to avoid overwhelming the system\n",
    "        if batch_num < total_batches:\n",
    "            print(f\"    ‚è≥ Waiting before next batch...\")\n",
    "            import time\n",
    "            time.sleep(2)\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\nüìä Batch Processing Summary:\")\n",
    "    print(f\"   ‚úÖ Successful: {len(successful_sections)} sections\")\n",
    "    if successful_sections:\n",
    "        print(f\"      Sections: {successful_sections}\")\n",
    "\n",
    "    print(f\"   ‚ùå Failed: {len(failed_sections)} sections\")\n",
    "    if failed_sections:\n",
    "        print(f\"      Sections: {failed_sections}\")\n",
    "\n",
    "    if EXECUTION_MODE == 'lambda':\n",
    "        print(f\"\\nüí° Monitor CloudWatch logs for processing status\")\n",
    "        print(f\"   Function: {XAPI_ETL_FUNCTION}\")\n",
    "        print(f\"   üîç Look for 'processing_method: s3_integration' entries\")\n",
    "\n",
    "else:\n",
    "    print(\"üí° No sections specified for processing\")\n",
    "    print(\"   Add section IDs to sections_to_process list above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec09df",
   "metadata": {},
   "source": [
    "## 6. Test Single File Processing\n",
    "\n",
    "Test processing of a single JSONL file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a specific file\n",
    "test_bucket = 'your-xapi-bucket'  # Replace with your S3 bucket\n",
    "test_key = 'section/123/video/2024-01-01T12-00-00.000Z_test-bundle.jsonl'  # Replace with actual file\n",
    "\n",
    "test_payload = {\n",
    "    'bucket': test_bucket,\n",
    "    'key': test_key\n",
    "}\n",
    "\n",
    "print(f\"Testing single file processing: s3://{test_bucket}/{test_key}\")\n",
    "\n",
    "test_result = execute_function(test_payload)\n",
    "\n",
    "if test_result['success']:\n",
    "    result_body = test_result['result']['body']\n",
    "    if isinstance(result_body, str):\n",
    "        result_data = json.loads(result_body)\n",
    "    else:\n",
    "        result_data = result_body\n",
    "\n",
    "    print(\"‚úÖ Single file processing completed\")\n",
    "    print(json.dumps(result_data, indent=2))\n",
    "else:\n",
    "    print(f\"‚ùå Single file processing failed: {test_result.get('error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d24d0",
   "metadata": {},
   "source": [
    "## 7. Monitoring and Troubleshooting\n",
    "\n",
    "Check Lambda function logs and status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check recent Lambda invocations (requires CloudWatch Logs access)\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "logs_client = boto3.client('logs', region_name=AWS_REGION)\n",
    "\n",
    "def get_recent_lambda_logs(function_name, hours=1):\n",
    "    \"\"\"Get recent logs for a Lambda function\"\"\"\n",
    "    log_group = f'/aws/lambda/{function_name}'\n",
    "\n",
    "    try:\n",
    "        end_time = datetime.utcnow()\n",
    "        start_time = end_time - timedelta(hours=hours)\n",
    "\n",
    "        response = logs_client.filter_log_events(\n",
    "            logGroupName=log_group,\n",
    "            startTime=int(start_time.timestamp() * 1000),\n",
    "            endTime=int(end_time.timestamp() * 1000),\n",
    "            limit=100\n",
    "        )\n",
    "\n",
    "        return response.get('events', [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting logs for {function_name}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Get recent logs for XAPI ETL processor\n",
    "print(f\"Recent logs for {XAPI_ETL_FUNCTION}:\")\n",
    "etl_logs = get_recent_lambda_logs(XAPI_ETL_FUNCTION)\n",
    "for event in etl_logs[-5:]:  # Show last 5 log events\n",
    "    timestamp = datetime.fromtimestamp(event['timestamp'] / 1000)\n",
    "    print(f\"[{timestamp}] {event['message']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
