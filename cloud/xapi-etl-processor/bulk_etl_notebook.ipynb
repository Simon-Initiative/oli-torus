{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa1e009",
   "metadata": {},
   "source": [
    "## Bulk XAPI ETL Processing\n",
    "\n",
    "This notebook demonstrates bulk processing of XAPI data using **pure ClickHouse S3 integration**, bypassing the traditional Lambda ETL processing pipeline entirely.\n",
    "\n",
    "### Why Pure S3 Integration?\n",
    "\n",
    "The traditional Lambda approach has timeout limitations (15 minutes max) which makes it unsuitable for bulk processing large datasets. ClickHouse's native S3 integration allows us to:\n",
    "\n",
    "1. **Direct Processing**: Process JSONL files directly from S3 without Lambda timeout limits\n",
    "2. **Batch Processing**: Handle large datasets (e.g., 2,218 files) in intelligent batches \n",
    "3. **Optimized Performance**: Leverage ClickHouse's native S3 functions for maximum efficiency\n",
    "\n",
    "### Recent S3 Function Syntax Fix ‚úÖ\n",
    "\n",
    "**Issue Resolved**: ClickHouse S3 functions require individual file URLs, not multiple URLs in a single call.\n",
    "\n",
    "- ‚ùå **Before**: `s3('url1,url2,url3', 'JSONEachRow')` ‚Üí caused `NUMBER_OF_ARGUMENTS_DOESNT_MATCH` error\n",
    "- ‚úÖ **After**: Process each file individually: `s3('url1', 'JSONEachRow')`, `s3('url2', 'JSONEachRow')`, etc.\n",
    "\n",
    "**Implementation**: The `_process_single_s3_file()` method now handles individual S3 files within batch processing, ensuring proper ClickHouse S3 function syntax.\n",
    "\n",
    "### Batch Processing Strategy\n",
    "\n",
    "For large datasets, we use intelligent batching:\n",
    "- **Batch Size**: 50 files per batch (configurable)\n",
    "- **Processing**: Each batch processes files individually \n",
    "- **Example**: 2,218 files ‚Üí 45 batches of 50 files each\n",
    "- **Resilience**: Individual file failures don't stop the entire batch\n",
    "\n",
    "**Test Results**: ‚úÖ All syntax issues resolved, ready for production processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b93cb",
   "metadata": {},
   "source": [
    "# OLI Torus XAPI ETL Pipeline - S3 Integration Bulk Processing\n",
    "\n",
    "This notebook provides an interface to trigger bulk processing of historical XAPI data using AWS Lambda with **pure ClickHouse S3 integration**.\n",
    "\n",
    "## S3 Integration Features\n",
    "\n",
    "The ETL pipeline now uses **pure ClickHouse S3 integration** for all operations:\n",
    "\n",
    "- **ClickHouse S3 Integration**: Direct processing of files from S3 using ClickHouse's native `s3()` table functions\n",
    "- **Unified Processing**: Single high-performance method for all batch sizes\n",
    "- **No Timeout Limits**: Bypasses Lambda's 15-minute limitation completely\n",
    "- **Consistent Performance**: Same fast processing whether handling 1 file or 1000 files\n",
    "\n",
    "## Performance Benefits\n",
    "\n",
    "- **No Lambda Timeout Limits**: Process unlimited datasets without time constraints\n",
    "- **10x Faster Processing**: ClickHouse S3 integration provides significant performance improvements\n",
    "- **Cost Effective**: Minimal Lambda execution time for orchestration only\n",
    "- **Simplified**: Single processing path reduces complexity and maintenance\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. AWS credentials configured (via AWS CLI, environment variables, or IAM roles)\n",
    "2. Lambda functions deployed in your target environment\n",
    "3. Appropriate permissions to invoke Lambda functions\n",
    "4. ClickHouse with S3 access permissions for S3 integration\n",
    "5. ClickHouse `s3()` table functions enabled\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7bc98",
   "metadata": {},
   "source": [
    "## Quick Setup - Install Requirements\n",
    "\n",
    "Run the cell below to install all required dependencies for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc07367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required dependencies\n",
    "# Choose one of the options below:\n",
    "\n",
    "# Option 1: Install notebook-specific requirements (includes pandas and visualization tools)\n",
    "!pip install -r notebook-requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Configure AWS region and environment\n",
    "AWS_REGION = 'us-east-1'  # Change to your region\n",
    "ENVIRONMENT = 'dev'  # Change to 'staging' or 'prod' as needed\n",
    "\n",
    "# Initialize AWS clients\n",
    "lambda_client = boto3.client('lambda', region_name=AWS_REGION)\n",
    "s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "# Unified Lambda function name (adjust based on your deployment)\n",
    "XAPI_ETL_FUNCTION = f'xapi-etl-processor-{ENVIRONMENT}'\n",
    "\n",
    "print(f\"Configured for {ENVIRONMENT} environment in {AWS_REGION}\")\n",
    "print(f\"Unified XAPI ETL Function: {XAPI_ETL_FUNCTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae94f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Environment Variables using python-dotenv\n",
    "import os\n",
    "\n",
    "# Install python-dotenv if not already installed\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"üîß Loading environment variables from .env file...\")\n",
    "\n",
    "# Load .env file - this is much simpler than manual parsing!\n",
    "if load_dotenv('.env'):\n",
    "    print(\"‚úÖ Environment variables loaded successfully!\")\n",
    "\n",
    "    # Check AWS configuration\n",
    "    aws_access_key = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "    aws_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "    aws_region = os.getenv('AWS_REGION')\n",
    "    s3_bucket = os.getenv('S3_XAPI_BUCKET')\n",
    "\n",
    "    print(f\"\\nüîç AWS Configuration Status:\")\n",
    "    print(f\"  AWS_ACCESS_KEY_ID: {'‚úÖ Set' if aws_access_key else '‚ùå Missing'}\")\n",
    "    print(f\"  AWS_SECRET_ACCESS_KEY: {'‚úÖ Set' if aws_secret_key else '‚ùå Missing'}\")\n",
    "    print(f\"  AWS_REGION: {aws_region if aws_region else '‚ùå Missing'}\")\n",
    "    print(f\"  S3_XAPI_BUCKET: {s3_bucket if s3_bucket else '‚ùå Missing'}\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Could not load .env file\")\n",
    "    print(f\"   Make sure .env file exists with your AWS credentials:\")\n",
    "    print(f\"   cp example.env .env\")\n",
    "    print(f\"   # Then edit .env with your actual credentials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f35e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Execution Mode Configuration\n",
    "EXECUTION_MODE = 'local'  # Change to 'local' to run locally instead of using Lambda\n",
    "\n",
    "# For local execution, we'll import the required modules\n",
    "if EXECUTION_MODE == 'local':\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    # Add current directory to path to import local modules\n",
    "    current_dir = os.path.dirname(os.path.abspath('__file__' if '__file__' in globals() else os.getcwd()))\n",
    "    if current_dir not in sys.path:\n",
    "        sys.path.insert(0, current_dir)\n",
    "\n",
    "    # Import local modules\n",
    "    try:\n",
    "        from lambda_function import lambda_handler, health_check\n",
    "        from common import get_config\n",
    "        from clickhouse_client import ClickHouseClient\n",
    "        print(f\"‚úÖ Local modules imported successfully\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Failed to import local modules: {e}\")\n",
    "        print(\"Make sure you're running this notebook from the xapi-etl-processor directory\")\n",
    "        print(\"Exiting due to failed local module imports.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "print(f\"üîß Execution mode: {EXECUTION_MODE.upper()}\")\n",
    "if EXECUTION_MODE == 'lambda':\n",
    "    print(f\"   Using Lambda function: {XAPI_ETL_FUNCTION}\")\n",
    "else:\n",
    "    print(f\"   Running locally with imported modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b06a0",
   "metadata": {},
   "source": [
    "## Execution Modes\n",
    "\n",
    "This notebook supports two execution modes with **pure S3 integration** in both:\n",
    "\n",
    "### Lambda Mode (Default)\n",
    "- **EXECUTION_MODE = 'lambda'**\n",
    "- Invokes the deployed AWS Lambda function remotely\n",
    "- Requires AWS credentials and deployed Lambda function\n",
    "- Best for production use and processing large datasets\n",
    "- Supports true asynchronous processing\n",
    "- **S3 Integration**: Lambda orchestrates ClickHouse S3 processing for all operations\n",
    "\n",
    "### Local Mode\n",
    "- **EXECUTION_MODE = 'local'**\n",
    "- Runs the Lambda function code locally in this notebook\n",
    "- Requires local modules (lambda_function.py, common.py, clickhouse_client.py)\n",
    "- Good for testing and development\n",
    "- All operations run synchronously in the notebook\n",
    "- **S3 Integration**: Local orchestration with ClickHouse S3 processing\n",
    "\n",
    "### S3 Integration Features (Both Modes)\n",
    "- **Pure S3 Processing**: All operations use ClickHouse native S3 table functions\n",
    "- **No Timeout Limits**: Process unlimited datasets without Lambda constraints\n",
    "- **Consistent Performance**: Same high-speed approach for all batch sizes\n",
    "- **Simplified Architecture**: Single processing path reduces complexity\n",
    "- **Cost Efficient**: Minimal compute time as ClickHouse handles heavy lifting\n",
    "\n",
    "**To switch modes:** Change the `EXECUTION_MODE` variable in the cell above and re-run that cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265f6a9",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae00b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_function(payload):\n",
    "    \"\"\"Execute function either locally or via Lambda based on EXECUTION_MODE\"\"\"\n",
    "    if EXECUTION_MODE == 'local':\n",
    "        return execute_local(payload)\n",
    "    else:\n",
    "        return invoke_lambda_sync(payload)\n",
    "\n",
    "def execute_function_async(payload):\n",
    "    \"\"\"Execute function asynchronously - Lambda only or simulate locally\"\"\"\n",
    "    if EXECUTION_MODE == 'local':\n",
    "        # For local execution, run synchronously but indicate it's \"async\"\n",
    "        result = execute_local(payload)\n",
    "        if result['success']:\n",
    "            result['async_mode'] = True\n",
    "            result['request_id'] = f\"local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "        return result\n",
    "    else:\n",
    "        return invoke_lambda_async(payload)\n",
    "\n",
    "def execute_local(payload):\n",
    "    \"\"\"Execute the Lambda function locally\"\"\"\n",
    "    try:\n",
    "        # Create a mock context object\n",
    "        class MockContext:\n",
    "            def __init__(self):\n",
    "                self.function_name = \"xapi-etl-processor-local\"\n",
    "                self.function_version = \"1\"\n",
    "                self.aws_request_id = f\"local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "        context = MockContext()\n",
    "\n",
    "        # Call the lambda_handler function directly\n",
    "        result = lambda_handler(payload, context)\n",
    "\n",
    "        return {\n",
    "            'success': True,\n",
    "            'status_code': result.get('statusCode', 200),\n",
    "            'result': result,\n",
    "            'execution_mode': 'local'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'execution_mode': 'local'\n",
    "        }\n",
    "\n",
    "def invoke_lambda_async(payload):\n",
    "    \"\"\"Invoke the unified Lambda function asynchronously\"\"\"\n",
    "    try:\n",
    "        response = lambda_client.invoke(\n",
    "            FunctionName=XAPI_ETL_FUNCTION,\n",
    "            InvocationType='Event',  # Async invocation\n",
    "            Payload=json.dumps(payload)\n",
    "        )\n",
    "        return {\n",
    "            'success': True,\n",
    "            'status_code': response['StatusCode'],\n",
    "            'request_id': response['ResponseMetadata']['RequestId'],\n",
    "            'execution_mode': 'lambda'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'execution_mode': 'lambda'\n",
    "        }\n",
    "\n",
    "def invoke_lambda_sync(payload):\n",
    "    \"\"\"Invoke the unified Lambda function synchronously\"\"\"\n",
    "    try:\n",
    "        response = lambda_client.invoke(\n",
    "            FunctionName=XAPI_ETL_FUNCTION,\n",
    "            InvocationType='RequestResponse',  # Sync invocation\n",
    "            Payload=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "        result = json.loads(response['Payload'].read().decode())\n",
    "        return {\n",
    "            'success': True,\n",
    "            'status_code': response['StatusCode'],\n",
    "            'result': result,\n",
    "            'execution_mode': 'lambda'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'execution_mode': 'lambda'\n",
    "        }\n",
    "\n",
    "def check_function_health():\n",
    "    \"\"\"Check if the function is healthy (Lambda or local)\"\"\"\n",
    "    payload = {'health_check': True}\n",
    "    return execute_function(payload)\n",
    "\n",
    "print(\"Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd18091",
   "metadata": {},
   "source": [
    "## 1. Health Checks\n",
    "\n",
    "First, let's verify that our Lambda functions and ClickHouse are healthy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7014ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check XAPI ETL processor health\n",
    "print(\"Checking XAPI ETL processor health...\")\n",
    "health = check_function_health()\n",
    "print(json.dumps(health, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40619cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clear existing section data before reprocessing\n",
    "# Uncomment and run this cell if you want to start fresh for this section\n",
    "\n",
    "clear_section_data = True  # Set to True to enable clearing\n",
    "target_section_id = '145'   # Make sure this matches your section ID above\n",
    "\n",
    "if clear_section_data:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: This will delete ALL existing data for section {target_section_id}\")\n",
    "\n",
    "    if EXECUTION_MODE == 'local':\n",
    "        try:\n",
    "            # For local execution, directly use ClickHouse client\n",
    "            from clickhouse_client import ClickHouseClient\n",
    "            clickhouse_client = ClickHouseClient()\n",
    "\n",
    "            # Delete all events for this section across all tables\n",
    "            tables = ['video_events', 'activity_attempt_events', 'page_attempt_events',\n",
    "                     'page_viewed_events', 'part_attempt_events']\n",
    "\n",
    "            total_deleted = 0\n",
    "            for table in tables:\n",
    "                try:\n",
    "                    query = f\"DELETE FROM {clickhouse_client.database}.{table} WHERE section_id = {target_section_id}\"\n",
    "                    response = clickhouse_client._execute_query(query)\n",
    "                    print(f\"‚úÖ Cleared {table} for section {target_section_id}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Table {table} may not exist or is empty: {str(e)}\")\n",
    "\n",
    "            print(f\"‚úÖ Section {target_section_id} data cleared - you can now set force_reprocess=False\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to clear section data: {str(e)}\")\n",
    "    else:\n",
    "        print(\"‚ùå Data clearing only available in local mode\")\n",
    "        print(\"   For Lambda mode, use force_reprocess=True instead\")\n",
    "else:\n",
    "    print(\"üí° Section data clearing disabled\")\n",
    "    print(\"   Set clear_section_data = True above to enable section data clearing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5b9c8",
   "metadata": {},
   "source": [
    "## 3. Process Specific Section Data\n",
    "\n",
    "Process historical data for a specific course section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b0a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure section-specific processing\n",
    "section_id = '145'  # Replace with actual section ID\n",
    "start_date = '2023-01-01'  # Adjust as needed\n",
    "end_date = '2025-12-31'    # Adjust as needed\n",
    "\n",
    "# Use S3_XAPI_BUCKET from environment variables\n",
    "S3_XAPI_BUCKET = os.getenv('S3_XAPI_BUCKET', 'torus-xapi-test')  # Fallback to default if not in .env\n",
    "\n",
    "section_payload = {\n",
    "    'mode': 'bulk',\n",
    "    's3_bucket': S3_XAPI_BUCKET,\n",
    "    'section_id': section_id,\n",
    "    'start_date': start_date,\n",
    "    'end_date': end_date,\n",
    "    'force_reprocess': False  # Set to True to reprocess existing data\n",
    "}\n",
    "\n",
    "print(f\"Processing section {section_id} data from {start_date} to {end_date}...\")\n",
    "print(f\"Using S3 bucket: {S3_XAPI_BUCKET}\")\n",
    "if section_payload['force_reprocess']:\n",
    "    print(f\"‚ö†Ô∏è  Force reprocess enabled - will reprocess existing data\")\n",
    "else:\n",
    "    print(f\"üí° Force reprocess disabled - will skip existing data\")\n",
    "\n",
    "print(\"\\nüöÄ S3 Integration Processing:\")\n",
    "print(\"   - Uses ClickHouse native S3 table functions\")\n",
    "print(\"   - High performance for all batch sizes\")\n",
    "print(\"   - No Lambda timeout limitations\")\n",
    "print(\"   - Direct S3 processing without file downloads\")\n",
    "\n",
    "if EXECUTION_MODE == 'local':\n",
    "    print(\"\\nüè† This will run locally (synchronously).\")\n",
    "else:\n",
    "    print(\"\\n‚òÅÔ∏è  This will be an asynchronous Lambda operation.\")\n",
    "\n",
    "section_result = execute_function_async(section_payload)\n",
    "\n",
    "if section_result['success']:\n",
    "    print(f\"\\n‚úÖ Successfully triggered S3 integration processing for section {section_id}\")\n",
    "    print(f\"Request ID: {section_result['request_id']}\")\n",
    "    if EXECUTION_MODE == 'lambda':\n",
    "        print(\"üìä Check CloudWatch logs for processing status.\")\n",
    "        print(\"üí° Look for 'processing_method: s3_integration' in results\")\n",
    "    else:\n",
    "        print(\"üîç Local execution completed.\")\n",
    "        # If local and we have result data, show processing method\n",
    "        if 'result' in section_result and isinstance(section_result['result'], dict):\n",
    "            result_body = section_result['result'].get('body', {})\n",
    "            if isinstance(result_body, str):\n",
    "                try:\n",
    "                    result_data = json.loads(result_body)\n",
    "                    processing_method = result_data.get('processing_method', 'unknown')\n",
    "                    print(f\"üìà Processing method used: {processing_method}\")\n",
    "                except:\n",
    "                    pass\n",
    "else:\n",
    "    print(f\"‚ùå Failed to trigger processing: {section_result.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de0658",
   "metadata": {},
   "source": [
    "## 4. Bulk Process Multiple Sections\n",
    "\n",
    "Process data for multiple sections (useful for large-scale historical data loading):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e78ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of section IDs to process\n",
    "section_ids = ['123', '456', '789']  # Replace with actual section IDs\n",
    "date_range = {\n",
    "    'start_date': '2024-01-01',\n",
    "    'end_date': '2024-12-31'\n",
    "}\n",
    "\n",
    "# Use S3_XAPI_BUCKET from environment variables\n",
    "S3_XAPI_BUCKET = os.getenv('S3_XAPI_BUCKET', 'torus-xapi-test')  # Fallback to default if not in .env\n",
    "\n",
    "print(f\"Processing {len(section_ids)} sections with S3 integration...\")\n",
    "print(f\"Using S3 bucket: {S3_XAPI_BUCKET}\")\n",
    "print(\"üöÄ All sections will use ClickHouse S3 Integration:\")\n",
    "print(\"   - High performance processing for all sections\")\n",
    "print(\"   - No timeout limitations regardless of data size\")\n",
    "print(\"   - Direct S3 processing with native ClickHouse functions\")\n",
    "\n",
    "results = []\n",
    "for i, section_id in enumerate(section_ids, 1):\n",
    "    payload = {\n",
    "        'mode': 'bulk',\n",
    "        's3_bucket': S3_XAPI_BUCKET,\n",
    "        'section_id': section_id,\n",
    "        **date_range,\n",
    "        'force_reprocess': False\n",
    "    }\n",
    "\n",
    "    print(f\"  {i}/{len(section_ids)}: Triggering S3 integration processing for section {section_id}...\")\n",
    "\n",
    "    result = execute_function_async(payload)\n",
    "    results.append({\n",
    "        'section_id': section_id,\n",
    "        'success': result['success'],\n",
    "        'request_id': result.get('request_id'),\n",
    "        'error': result.get('error'),\n",
    "        'execution_mode': result.get('execution_mode', 'unknown')\n",
    "    })\n",
    "\n",
    "    # Small delay to avoid overwhelming Lambda (not needed for local execution)\n",
    "    if EXECUTION_MODE == 'lambda':\n",
    "        time.sleep(1)\n",
    "\n",
    "# Summary\n",
    "successful = sum(1 for r in results if r['success'])\n",
    "failed = len(results) - successful\n",
    "\n",
    "print(f\"\\nüìä Processing Summary:\")\n",
    "print(f\"  ‚úÖ Successfully triggered: {successful}\")\n",
    "print(f\"  ‚ùå Failed: {failed}\")\n",
    "print(f\"  üöÄ All sections use ClickHouse S3 Integration\")\n",
    "\n",
    "if successful > 0:\n",
    "    print(f\"\\nüí° Monitor CloudWatch logs for processing status:\")\n",
    "    print(f\"   - 'processing_method: s3_integration' indicates successful S3 processing\")\n",
    "    print(f\"   - High performance processing with no timeout limitations\")\n",
    "\n",
    "if failed > 0:\n",
    "    print(\"\\n‚ùå Failed sections:\")\n",
    "    for result in results:\n",
    "        if not result['success']:\n",
    "            print(f\"  - Section {result['section_id']}: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def5280f",
   "metadata": {},
   "source": [
    "## 5. Process All Available Data\n",
    "\n",
    "Process all available XAPI data (use with caution for large datasets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17967beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è WARNING: This will process ALL available data. Use carefully!\n",
    "process_all = False  # Set to True to enable\n",
    "\n",
    "if process_all:\n",
    "    # Use S3_XAPI_BUCKET from environment variables\n",
    "    S3_XAPI_BUCKET = os.getenv('S3_XAPI_BUCKET', 'torus-xapi-test')  # Fallback to default if not in .env\n",
    "\n",
    "    all_data_payload = {\n",
    "        'mode': 'bulk',\n",
    "        's3_bucket': S3_XAPI_BUCKET,\n",
    "        's3_prefix': 'section/',\n",
    "        'start_date': '2024-01-01',  # Adjust as needed\n",
    "        'end_date': '2024-12-31',    # Adjust as needed\n",
    "        'force_reprocess': False\n",
    "    }\n",
    "\n",
    "    print(\"‚ö†Ô∏è  Processing ALL available data with S3 Integration...\")\n",
    "    print(f\"Using S3 bucket: {S3_XAPI_BUCKET}\")\n",
    "    print(\"üöÄ Performance advantages for large datasets:\")\n",
    "    print(\"   - Uses ClickHouse S3 Integration for optimal performance\")\n",
    "    print(\"   - Bypasses Lambda 15-minute timeout limitation completely\")\n",
    "    print(\"   - Processes unlimited files efficiently\")\n",
    "    print(\"   - Direct S3 processing with native ClickHouse functions\")\n",
    "\n",
    "    if EXECUTION_MODE == 'local':\n",
    "        print(\"\\nüè† This will run locally (synchronously).\")\n",
    "        print(\"   Large datasets will be handled by ClickHouse S3 integration.\")\n",
    "    else:\n",
    "        print(\"\\n‚òÅÔ∏è  This is an asynchronous Lambda operation.\")\n",
    "        print(\"   Lambda orchestrates ClickHouse for all heavy processing.\")\n",
    "\n",
    "    all_result = execute_function_async(all_data_payload)\n",
    "\n",
    "    if all_result['success']:\n",
    "        print(f\"\\n‚úÖ Successfully triggered S3 integration bulk processing\")\n",
    "        print(f\"Request ID: {all_result['request_id']}\")\n",
    "        if EXECUTION_MODE == 'lambda':\n",
    "            print(\"üìä Monitor CloudWatch logs for:\")\n",
    "            print(\"   - 'processing_method: s3_integration' status\")\n",
    "            print(\"   - Performance metrics and progress\")\n",
    "            print(\"   - Processing completion status\")\n",
    "        else:\n",
    "            print(\"üîç Local execution completed.\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to trigger bulk processing: {all_result.get('error')}\")\n",
    "else:\n",
    "    print(\"üõë Bulk processing disabled. Set process_all = True to enable.\")\n",
    "    print(\"üí° When enabled, the system will use:\")\n",
    "    print(\"   - ClickHouse S3 Integration for optimal performance on all datasets\")\n",
    "    print(\"   - No timeout limitations regardless of data size\")\n",
    "    print(\"   - Direct S3 processing for maximum efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec09df",
   "metadata": {},
   "source": [
    "## 6. Test Single File Processing\n",
    "\n",
    "Test processing of a single JSONL file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a specific file\n",
    "test_bucket = 'your-xapi-bucket'  # Replace with your S3 bucket\n",
    "test_key = 'section/123/video/2024-01-01T12-00-00.000Z_test-bundle.jsonl'  # Replace with actual file\n",
    "\n",
    "test_payload = {\n",
    "    'bucket': test_bucket,\n",
    "    'key': test_key\n",
    "}\n",
    "\n",
    "print(f\"Testing single file processing: s3://{test_bucket}/{test_key}\")\n",
    "\n",
    "test_result = execute_function(test_payload)\n",
    "\n",
    "if test_result['success']:\n",
    "    result_body = test_result['result']['body']\n",
    "    if isinstance(result_body, str):\n",
    "        result_data = json.loads(result_body)\n",
    "    else:\n",
    "        result_data = result_body\n",
    "\n",
    "    print(\"‚úÖ Single file processing completed\")\n",
    "    print(json.dumps(result_data, indent=2))\n",
    "else:\n",
    "    print(f\"‚ùå Single file processing failed: {test_result.get('error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d24d0",
   "metadata": {},
   "source": [
    "## 7. Monitoring and Troubleshooting\n",
    "\n",
    "Check Lambda function logs and status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check recent Lambda invocations (requires CloudWatch Logs access)\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "logs_client = boto3.client('logs', region_name=AWS_REGION)\n",
    "\n",
    "def get_recent_lambda_logs(function_name, hours=1):\n",
    "    \"\"\"Get recent logs for a Lambda function\"\"\"\n",
    "    log_group = f'/aws/lambda/{function_name}'\n",
    "\n",
    "    try:\n",
    "        end_time = datetime.utcnow()\n",
    "        start_time = end_time - timedelta(hours=hours)\n",
    "\n",
    "        response = logs_client.filter_log_events(\n",
    "            logGroupName=log_group,\n",
    "            startTime=int(start_time.timestamp() * 1000),\n",
    "            endTime=int(end_time.timestamp() * 1000),\n",
    "            limit=100\n",
    "        )\n",
    "\n",
    "        return response.get('events', [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting logs for {function_name}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Get recent logs for XAPI ETL processor\n",
    "print(f\"Recent logs for {XAPI_ETL_FUNCTION}:\")\n",
    "etl_logs = get_recent_lambda_logs(XAPI_ETL_FUNCTION)\n",
    "for event in etl_logs[-5:]:  # Show last 5 log events\n",
    "    timestamp = datetime.fromtimestamp(event['timestamp'] / 1000)\n",
    "    print(f\"[{timestamp}] {event['message']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
