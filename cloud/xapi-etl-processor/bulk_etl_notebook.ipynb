{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21b93cb",
   "metadata": {},
   "source": [
    "# OLI Torus XAPI ETL Pipeline - Bulk Processing\n",
    "\n",
    "This notebook provides an interface to trigger bulk processing of historical XAPI data using AWS Lambda.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. AWS credentials configured (via AWS CLI, environment variables, or IAM roles)\n",
    "2. Lambda functions deployed in your target environment\n",
    "3. Appropriate permissions to invoke Lambda functions\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7bc98",
   "metadata": {},
   "source": [
    "## Quick Setup - Install Requirements\n",
    "\n",
    "Run the cell below to install all required dependencies for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc07367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required dependencies\n",
    "# Choose one of the options below:\n",
    "\n",
    "# Option 1: Install notebook-specific requirements (includes pandas and visualization tools)\n",
    "!pip install -r notebook-requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Configure AWS region and environment\n",
    "AWS_REGION = 'us-east-1'  # Change to your region\n",
    "ENVIRONMENT = 'dev'  # Change to 'staging' or 'prod' as needed\n",
    "\n",
    "# Initialize AWS clients\n",
    "lambda_client = boto3.client('lambda', region_name=AWS_REGION)\n",
    "s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "# Unified Lambda function name (adjust based on your deployment)\n",
    "XAPI_ETL_FUNCTION = f'xapi-etl-processor-{ENVIRONMENT}'\n",
    "\n",
    "print(f\"Configured for {ENVIRONMENT} environment in {AWS_REGION}\")\n",
    "print(f\"Unified XAPI ETL Function: {XAPI_ETL_FUNCTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a4049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Configuration\n",
    "S3_XAPI_BUCKET = 'torus-xapi-test'  # Replace with your actual S3 bucket name\n",
    "\n",
    "print(f\"S3 Bucket: {S3_XAPI_BUCKET}\")\n",
    "print(\"‚ö†Ô∏è  Make sure to update S3_XAPI_BUCKET with your actual bucket name!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f35e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Execution Mode Configuration\n",
    "EXECUTION_MODE = 'local'  # Change to 'local' to run locally instead of using Lambda\n",
    "\n",
    "# For local execution, we'll import the required modules\n",
    "if EXECUTION_MODE == 'local':\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    # Add current directory to path to import local modules\n",
    "    current_dir = os.path.dirname(os.path.abspath('__file__' if '__file__' in globals() else os.getcwd()))\n",
    "    if current_dir not in sys.path:\n",
    "        sys.path.insert(0, current_dir)\n",
    "\n",
    "    # Import local modules\n",
    "    try:\n",
    "        from lambda_function import lambda_handler, health_check\n",
    "        from common import get_config\n",
    "        from clickhouse_client import ClickHouseClient\n",
    "        print(f\"‚úÖ Local modules imported successfully\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Failed to import local modules: {e}\")\n",
    "        print(\"Make sure you're running this notebook from the xapi-etl-processor directory\")\n",
    "        print(\"Exiting due to failed local module imports.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "print(f\"üîß Execution mode: {EXECUTION_MODE.upper()}\")\n",
    "if EXECUTION_MODE == 'lambda':\n",
    "    print(f\"   Using Lambda function: {XAPI_ETL_FUNCTION}\")\n",
    "else:\n",
    "    print(f\"   Running locally with imported modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b06a0",
   "metadata": {},
   "source": [
    "## Execution Modes\n",
    "\n",
    "This notebook supports two execution modes:\n",
    "\n",
    "### Lambda Mode (Default)\n",
    "- **EXECUTION_MODE = 'lambda'**\n",
    "- Invokes the deployed AWS Lambda function remotely\n",
    "- Requires AWS credentials and deployed Lambda function\n",
    "- Best for production use and processing large datasets\n",
    "- Supports true asynchronous processing\n",
    "\n",
    "### Local Mode\n",
    "- **EXECUTION_MODE = 'local'**\n",
    "- Runs the Lambda function code locally in this notebook\n",
    "- Requires local modules (lambda_function.py, common.py, clickhouse_client.py)\n",
    "- Good for testing and development\n",
    "- All operations run synchronously in the notebook\n",
    "\n",
    "**To switch modes:** Change the `EXECUTION_MODE` variable in the cell above and re-run that cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265f6a9",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae00b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_function(payload):\n",
    "    \"\"\"Execute function either locally or via Lambda based on EXECUTION_MODE\"\"\"\n",
    "    if EXECUTION_MODE == 'local':\n",
    "        return execute_local(payload)\n",
    "    else:\n",
    "        return invoke_lambda_sync(payload)\n",
    "\n",
    "def execute_function_async(payload):\n",
    "    \"\"\"Execute function asynchronously - Lambda only or simulate locally\"\"\"\n",
    "    if EXECUTION_MODE == 'local':\n",
    "        # For local execution, run synchronously but indicate it's \"async\"\n",
    "        result = execute_local(payload)\n",
    "        if result['success']:\n",
    "            result['async_mode'] = True\n",
    "            result['request_id'] = f\"local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "        return result\n",
    "    else:\n",
    "        return invoke_lambda_async(payload)\n",
    "\n",
    "def execute_local(payload):\n",
    "    \"\"\"Execute the Lambda function locally\"\"\"\n",
    "    try:\n",
    "        # Create a mock context object\n",
    "        class MockContext:\n",
    "            def __init__(self):\n",
    "                self.function_name = \"xapi-etl-processor-local\"\n",
    "                self.function_version = \"1\"\n",
    "                self.aws_request_id = f\"local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "        context = MockContext()\n",
    "\n",
    "        # Call the lambda_handler function directly\n",
    "        result = lambda_handler(payload, context)\n",
    "\n",
    "        return {\n",
    "            'success': True,\n",
    "            'status_code': result.get('statusCode', 200),\n",
    "            'result': result,\n",
    "            'execution_mode': 'local'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'execution_mode': 'local'\n",
    "        }\n",
    "\n",
    "def invoke_lambda_async(payload):\n",
    "    \"\"\"Invoke the unified Lambda function asynchronously\"\"\"\n",
    "    try:\n",
    "        response = lambda_client.invoke(\n",
    "            FunctionName=XAPI_ETL_FUNCTION,\n",
    "            InvocationType='Event',  # Async invocation\n",
    "            Payload=json.dumps(payload)\n",
    "        )\n",
    "        return {\n",
    "            'success': True,\n",
    "            'status_code': response['StatusCode'],\n",
    "            'request_id': response['ResponseMetadata']['RequestId'],\n",
    "            'execution_mode': 'lambda'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'execution_mode': 'lambda'\n",
    "        }\n",
    "\n",
    "def invoke_lambda_sync(payload):\n",
    "    \"\"\"Invoke the unified Lambda function synchronously\"\"\"\n",
    "    try:\n",
    "        response = lambda_client.invoke(\n",
    "            FunctionName=XAPI_ETL_FUNCTION,\n",
    "            InvocationType='RequestResponse',  # Sync invocation\n",
    "            Payload=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "        result = json.loads(response['Payload'].read().decode())\n",
    "        return {\n",
    "            'success': True,\n",
    "            'status_code': response['StatusCode'],\n",
    "            'result': result,\n",
    "            'execution_mode': 'lambda'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'execution_mode': 'lambda'\n",
    "        }\n",
    "\n",
    "def check_function_health():\n",
    "    \"\"\"Check if the function is healthy (Lambda or local)\"\"\"\n",
    "    payload = {'health_check': True}\n",
    "    return execute_function(payload)\n",
    "\n",
    "print(\"Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd18091",
   "metadata": {},
   "source": [
    "## 1. Health Checks\n",
    "\n",
    "First, let's verify that our Lambda functions and ClickHouse are healthy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7014ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check XAPI ETL processor health\n",
    "print(\"Checking XAPI ETL processor health...\")\n",
    "health = check_function_health()\n",
    "print(json.dumps(health, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c86d4",
   "metadata": {},
   "source": [
    "## 2. Dry Run - Explore Available Data\n",
    "\n",
    "Let's do a dry run to see what data is available for processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ddb18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for dry run\n",
    "dry_run_payload = {\n",
    "    'mode': 'bulk',\n",
    "    's3_bucket': S3_XAPI_BUCKET,\n",
    "    's3_prefix': 'section/',  # Adjust based on your S3 structure\n",
    "    'start_date': '2024-01-01',  # Adjust date range as needed\n",
    "    'end_date': '2024-12-31',\n",
    "    'dry_run': True\n",
    "}\n",
    "\n",
    "print(\"Running dry run to explore available data...\")\n",
    "dry_run_result = execute_function(dry_run_payload)\n",
    "\n",
    "if dry_run_result['success']:\n",
    "    result_body = dry_run_result['result']['body']\n",
    "    if isinstance(result_body, str):\n",
    "        result_data = json.loads(result_body)\n",
    "    else:\n",
    "        result_data = result_body\n",
    "\n",
    "    print(f\"Found {result_data.get('files_found', 0)} files to process\")\n",
    "    print(f\"Total files: {result_data.get('total_files', 0)}\")\n",
    "\n",
    "    if 'files' in result_data:\n",
    "        print(\"\\nSample files:\")\n",
    "        for i, file in enumerate(result_data['files'][:5], 1):\n",
    "            print(f\"  {i}. {file}\")\n",
    "else:\n",
    "    print(f\"Dry run failed: {dry_run_result.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40619cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clear existing section data before reprocessing\n",
    "# Uncomment and run this cell if you want to start fresh for this section\n",
    "\n",
    "clear_section_data = True  # Set to True to enable clearing\n",
    "target_section_id = '145'   # Make sure this matches your section ID above\n",
    "\n",
    "if clear_section_data:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: This will delete ALL existing data for section {target_section_id}\")\n",
    "\n",
    "    if EXECUTION_MODE == 'local':\n",
    "        try:\n",
    "            # For local execution, directly use ClickHouse client\n",
    "            from clickhouse_client import ClickHouseClient\n",
    "            clickhouse_client = ClickHouseClient()\n",
    "\n",
    "            # Delete all events for this section across all tables\n",
    "            tables = ['video_events', 'activity_attempt_events', 'page_attempt_events',\n",
    "                     'page_viewed_events', 'part_attempt_events']\n",
    "\n",
    "            total_deleted = 0\n",
    "            for table in tables:\n",
    "                try:\n",
    "                    query = f\"DELETE FROM {clickhouse_client.database}.{table} WHERE section_id = {target_section_id}\"\n",
    "                    response = clickhouse_client._execute_query(query)\n",
    "                    print(f\"‚úÖ Cleared {table} for section {target_section_id}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Table {table} may not exist or is empty: {str(e)}\")\n",
    "\n",
    "            print(f\"‚úÖ Section {target_section_id} data cleared - you can now set force_reprocess=False\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to clear section data: {str(e)}\")\n",
    "    else:\n",
    "        print(\"‚ùå Data clearing only available in local mode\")\n",
    "        print(\"   For Lambda mode, use force_reprocess=True instead\")\n",
    "else:\n",
    "    print(\"üí° Section data clearing disabled\")\n",
    "    print(\"   Set clear_section_data = True above to enable section data clearing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5b9c8",
   "metadata": {},
   "source": [
    "## 3. Process Specific Section Data\n",
    "\n",
    "Process historical data for a specific course section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b0a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure section-specific processing\n",
    "section_id = '145'  # Replace with actual section ID\n",
    "start_date = '2023-01-01'  # Adjust as needed\n",
    "end_date = '2025-12-31'    # Adjust as needed\n",
    "\n",
    "section_payload = {\n",
    "    'mode': 'bulk',\n",
    "    's3_bucket': S3_XAPI_BUCKET,\n",
    "    'section_id': section_id,\n",
    "    'start_date': start_date,\n",
    "    'end_date': end_date,\n",
    "    'force_reprocess': True  # Set to True to reprocess existing data - CHANGED!\n",
    "}\n",
    "\n",
    "print(f\"Processing section {section_id} data from {start_date} to {end_date}...\")\n",
    "print(f\"‚ö†Ô∏è  Force reprocess enabled - will reprocess existing data\")\n",
    "if EXECUTION_MODE == 'local':\n",
    "    print(\"This will run locally (synchronously).\")\n",
    "else:\n",
    "    print(\"This will be an asynchronous operation.\")\n",
    "\n",
    "section_result = execute_function_async(section_payload)\n",
    "\n",
    "if section_result['success']:\n",
    "    print(f\"‚úÖ Successfully triggered processing for section {section_id}\")\n",
    "    print(f\"Request ID: {section_result['request_id']}\")\n",
    "    if EXECUTION_MODE == 'lambda':\n",
    "        print(\"Check CloudWatch logs for processing status.\")\n",
    "    else:\n",
    "        print(\"Local execution completed.\")\n",
    "else:\n",
    "    print(f\"‚ùå Failed to trigger processing: {section_result.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de0658",
   "metadata": {},
   "source": [
    "## 4. Bulk Process Multiple Sections\n",
    "\n",
    "Process data for multiple sections (useful for large-scale historical data loading):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e78ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of section IDs to process\n",
    "section_ids = ['123', '456', '789']  # Replace with actual section IDs\n",
    "date_range = {\n",
    "    'start_date': '2024-01-01',\n",
    "    'end_date': '2024-12-31'\n",
    "}\n",
    "\n",
    "print(f\"Processing {len(section_ids)} sections...\")\n",
    "\n",
    "results = []\n",
    "for i, section_id in enumerate(section_ids, 1):\n",
    "    payload = {\n",
    "        'mode': 'bulk',\n",
    "        's3_bucket': S3_XAPI_BUCKET,\n",
    "        'section_id': section_id,\n",
    "        **date_range,\n",
    "        'force_reprocess': False\n",
    "    }\n",
    "\n",
    "    print(f\"  {i}/{len(section_ids)}: Triggering processing for section {section_id}...\")\n",
    "\n",
    "    result = execute_function_async(payload)\n",
    "    results.append({\n",
    "        'section_id': section_id,\n",
    "        'success': result['success'],\n",
    "        'request_id': result.get('request_id'),\n",
    "        'error': result.get('error')\n",
    "    })\n",
    "\n",
    "    # Small delay to avoid overwhelming Lambda (not needed for local execution)\n",
    "    if EXECUTION_MODE == 'lambda':\n",
    "        time.sleep(1)\n",
    "\n",
    "# Summary\n",
    "successful = sum(1 for r in results if r['success'])\n",
    "failed = len(results) - successful\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  ‚úÖ Successfully triggered: {successful}\")\n",
    "print(f\"  ‚ùå Failed: {failed}\")\n",
    "\n",
    "if failed > 0:\n",
    "    print(\"\\nFailed sections:\")\n",
    "    for result in results:\n",
    "        if not result['success']:\n",
    "            print(f\"  - Section {result['section_id']}: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def5280f",
   "metadata": {},
   "source": [
    "## 5. Process All Available Data\n",
    "\n",
    "Process all available XAPI data (use with caution for large datasets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17967beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è WARNING: This will process ALL available data. Use carefully!\n",
    "process_all = False  # Set to True to enable\n",
    "\n",
    "if process_all:\n",
    "    all_data_payload = {\n",
    "        'mode': 'bulk',\n",
    "        's3_bucket': S3_XAPI_BUCKET,\n",
    "        's3_prefix': 'section/',\n",
    "        'start_date': '2024-01-01',  # Adjust as needed\n",
    "        'end_date': '2024-12-31',    # Adjust as needed\n",
    "        'force_reprocess': False\n",
    "    }\n",
    "\n",
    "    print(\"‚ö†Ô∏è  Processing ALL available data...\")\n",
    "    if EXECUTION_MODE == 'local':\n",
    "        print(\"This will run locally (synchronously).\")\n",
    "    else:\n",
    "        print(\"This is an asynchronous operation that may take a long time.\")\n",
    "\n",
    "    all_result = execute_function_async(all_data_payload)\n",
    "\n",
    "    if all_result['success']:\n",
    "        print(f\"‚úÖ Successfully triggered bulk processing\")\n",
    "        print(f\"Request ID: {all_result['request_id']}\")\n",
    "        if EXECUTION_MODE == 'lambda':\n",
    "            print(\"Monitor CloudWatch logs for progress.\")\n",
    "        else:\n",
    "            print(\"Local execution completed.\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to trigger bulk processing: {all_result.get('error')}\")\n",
    "else:\n",
    "    print(\"Bulk processing disabled. Set process_all = True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec09df",
   "metadata": {},
   "source": [
    "## 6. Test Single File Processing\n",
    "\n",
    "Test processing of a single JSONL file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a specific file\n",
    "test_bucket = 'your-xapi-bucket'  # Replace with your S3 bucket\n",
    "test_key = 'section/123/video/2024-01-01T12-00-00.000Z_test-bundle.jsonl'  # Replace with actual file\n",
    "\n",
    "test_payload = {\n",
    "    'bucket': test_bucket,\n",
    "    'key': test_key\n",
    "}\n",
    "\n",
    "print(f\"Testing single file processing: s3://{test_bucket}/{test_key}\")\n",
    "\n",
    "test_result = execute_function(test_payload)\n",
    "\n",
    "if test_result['success']:\n",
    "    result_body = test_result['result']['body']\n",
    "    if isinstance(result_body, str):\n",
    "        result_data = json.loads(result_body)\n",
    "    else:\n",
    "        result_data = result_body\n",
    "\n",
    "    print(\"‚úÖ Single file processing completed\")\n",
    "    print(json.dumps(result_data, indent=2))\n",
    "else:\n",
    "    print(f\"‚ùå Single file processing failed: {test_result.get('error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d24d0",
   "metadata": {},
   "source": [
    "## 7. Monitoring and Troubleshooting\n",
    "\n",
    "Check Lambda function logs and status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check recent Lambda invocations (requires CloudWatch Logs access)\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "logs_client = boto3.client('logs', region_name=AWS_REGION)\n",
    "\n",
    "def get_recent_lambda_logs(function_name, hours=1):\n",
    "    \"\"\"Get recent logs for a Lambda function\"\"\"\n",
    "    log_group = f'/aws/lambda/{function_name}'\n",
    "\n",
    "    try:\n",
    "        end_time = datetime.utcnow()\n",
    "        start_time = end_time - timedelta(hours=hours)\n",
    "\n",
    "        response = logs_client.filter_log_events(\n",
    "            logGroupName=log_group,\n",
    "            startTime=int(start_time.timestamp() * 1000),\n",
    "            endTime=int(end_time.timestamp() * 1000),\n",
    "            limit=100\n",
    "        )\n",
    "\n",
    "        return response.get('events', [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting logs for {function_name}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Get recent logs for XAPI ETL processor\n",
    "print(f\"Recent logs for {XAPI_ETL_FUNCTION}:\")\n",
    "etl_logs = get_recent_lambda_logs(XAPI_ETL_FUNCTION)\n",
    "for event in etl_logs[-5:]:  # Show last 5 log events\n",
    "    timestamp = datetime.fromtimestamp(event['timestamp'] / 1000)\n",
    "    print(f\"[{timestamp}] {event['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e36ff",
   "metadata": {},
   "source": [
    "## 8. ClickHouse Data Verification\n",
    "\n",
    "If you have direct access to ClickHouse, you can verify the data was loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This requires direct ClickHouse access or a separate verification Lambda\n",
    "# Here's sample code for direct verification (uncomment and modify as needed)\n",
    "\n",
    "import requests\n",
    "\n",
    "# ClickHouse connection details\n",
    "CLICKHOUSE_HOST = 'localhost'\n",
    "CLICKHOUSE_PORT = 8123\n",
    "CLICKHOUSE_USER = 'default'\n",
    "CLICKHOUSE_PASSWORD = 'clickhouse'\n",
    "CLICKHOUSE_DATABASE = 'oli_analytics_dev'\n",
    "\n",
    "def query_clickhouse(query):\n",
    "    url = f\"http://{CLICKHOUSE_HOST}:{CLICKHOUSE_PORT}\"\n",
    "    headers = {\n",
    "        'Content-Type': 'text/plain',\n",
    "        'X-ClickHouse-User': CLICKHOUSE_USER,\n",
    "        'X-ClickHouse-Key': CLICKHOUSE_PASSWORD\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, data=query, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\"\n",
    "\n",
    "# Check total event count\n",
    "total_events_query = f\"SELECT COUNT(*) FROM {CLICKHOUSE_DATABASE}.video_events\"\n",
    "total_events = query_clickhouse(total_events_query)\n",
    "print(f\"Total video events in ClickHouse: {total_events}\")\n",
    "\n",
    "# Check events by section\n",
    "section_query = f'''\n",
    "SELECT section_id, COUNT(*) as event_count\n",
    "FROM {CLICKHOUSE_DATABASE}.video_events\n",
    "GROUP BY section_id\n",
    "ORDER BY event_count DESC\n",
    "LIMIT 10\n",
    "'''\n",
    "section_stats = query_clickhouse(section_query)\n",
    "print(f\"\\\\nEvents by section (top 10):\\\\n{section_stats}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
