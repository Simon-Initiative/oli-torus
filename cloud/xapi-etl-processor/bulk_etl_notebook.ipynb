{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21b93cb",
   "metadata": {},
   "source": [
    "# OLI Torus XAPI ETL Pipeline - Bulk Processing\n",
    "\n",
    "This notebook provides an interface to trigger bulk processing of historical XAPI data using AWS Lambda.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. AWS credentials configured (via AWS CLI, environment variables, or IAM roles)\n",
    "2. Lambda functions deployed in your target environment\n",
    "3. Appropriate permissions to invoke Lambda functions\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Configure AWS region and environment\n",
    "AWS_REGION = 'us-east-1'  # Change to your region\n",
    "ENVIRONMENT = 'dev'  # Change to 'staging' or 'prod' as needed\n",
    "\n",
    "# Initialize AWS clients\n",
    "lambda_client = boto3.client('lambda', region_name=AWS_REGION)\n",
    "s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "# Unified Lambda function name (adjust based on your deployment)\n",
    "XAPI_ETL_FUNCTION = f'xapi-etl-processor-{ENVIRONMENT}'\n",
    "\n",
    "print(f\"Configured for {ENVIRONMENT} environment in {AWS_REGION}\")\n",
    "print(f\"Unified XAPI ETL Function: {XAPI_ETL_FUNCTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265f6a9",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae00b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_lambda_async(payload):\n",
    "    \"\"\"Invoke the unified Lambda function asynchronously\"\"\"\n",
    "    try:\n",
    "        response = lambda_client.invoke(\n",
    "            FunctionName=XAPI_ETL_FUNCTION,\n",
    "            InvocationType='Event',  # Async invocation\n",
    "            Payload=json.dumps(payload)\n",
    "        )\n",
    "        return {\n",
    "            'success': True,\n",
    "            'status_code': response['StatusCode'],\n",
    "            'request_id': response['ResponseMetadata']['RequestId']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def invoke_lambda_sync(payload):\n",
    "    \"\"\"Invoke the unified Lambda function synchronously\"\"\"\n",
    "    try:\n",
    "        response = lambda_client.invoke(\n",
    "            FunctionName=XAPI_ETL_FUNCTION,\n",
    "            InvocationType='RequestResponse',  # Sync invocation\n",
    "            Payload=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "        result = json.loads(response['Payload'].read().decode())\n",
    "        return {\n",
    "            'success': True,\n",
    "            'status_code': response['StatusCode'],\n",
    "            'result': result\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def check_function_health():\n",
    "    \"\"\"Check if the unified Lambda function is healthy\"\"\"\n",
    "    payload = {'health_check': True}\n",
    "    result = invoke_lambda_sync(payload)\n",
    "    return result\n",
    "\n",
    "print(\"Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd18091",
   "metadata": {},
   "source": [
    "## 1. Health Checks\n",
    "\n",
    "First, let's verify that our Lambda functions and ClickHouse are healthy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7014ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check XAPI ETL processor health\n",
    "print(\"Checking XAPI ETL processor health...\")\n",
    "health = check_function_health()\n",
    "print(json.dumps(health, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c86d4",
   "metadata": {},
   "source": [
    "## 2. Dry Run - Explore Available Data\n",
    "\n",
    "Let's do a dry run to see what data is available for processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ddb18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for dry run\n",
    "dry_run_payload = {\n",
    "    'mode': 'bulk',\n",
    "    's3_prefix': 'section/',  # Adjust based on your S3 structure\n",
    "    'start_date': '2024-01-01',  # Adjust date range as needed\n",
    "    'end_date': '2024-12-31',\n",
    "    'dry_run': True\n",
    "}\n",
    "\n",
    "print(\"Running dry run to explore available data...\")\n",
    "dry_run_result = invoke_lambda_sync(dry_run_payload)\n",
    "\n",
    "if dry_run_result['success']:\n",
    "    result_body = dry_run_result['result']['body']\n",
    "    if isinstance(result_body, str):\n",
    "        result_data = json.loads(result_body)\n",
    "    else:\n",
    "        result_data = result_body\n",
    "\n",
    "    print(f\"Found {result_data.get('files_found', 0)} files to process\")\n",
    "    print(f\"Total files: {result_data.get('total_files', 0)}\")\n",
    "\n",
    "    if 'files' in result_data:\n",
    "        print(\"\\nSample files:\")\n",
    "        for i, file in enumerate(result_data['files'][:5], 1):\n",
    "            print(f\"  {i}. {file}\")\n",
    "else:\n",
    "    print(f\"Dry run failed: {dry_run_result.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5b9c8",
   "metadata": {},
   "source": [
    "## 3. Process Specific Section Data\n",
    "\n",
    "Process historical data for a specific course section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b0a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure section-specific processing\n",
    "section_id = '123'  # Replace with actual section ID\n",
    "start_date = '2024-01-01'  # Adjust as needed\n",
    "end_date = '2024-12-31'    # Adjust as needed\n",
    "\n",
    "section_payload = {\n",
    "    'mode': 'bulk',\n",
    "    'section_id': section_id,\n",
    "    'start_date': start_date,\n",
    "    'end_date': end_date,\n",
    "    'force_reprocess': False  # Set to True to reprocess existing data\n",
    "}\n",
    "\n",
    "print(f\"Processing section {section_id} data from {start_date} to {end_date}...\")\n",
    "print(\"This will be an asynchronous operation.\")\n",
    "\n",
    "section_result = invoke_lambda_async(section_payload)\n",
    "\n",
    "if section_result['success']:\n",
    "    print(f\"‚úÖ Successfully triggered processing for section {section_id}\")\n",
    "    print(f\"Request ID: {section_result['request_id']}\")\n",
    "    print(\"Check CloudWatch logs for processing status.\")\n",
    "else:\n",
    "    print(f\"‚ùå Failed to trigger processing: {section_result.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de0658",
   "metadata": {},
   "source": [
    "## 4. Bulk Process Multiple Sections\n",
    "\n",
    "Process data for multiple sections (useful for large-scale historical data loading):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e78ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of section IDs to process\n",
    "section_ids = ['123', '456', '789']  # Replace with actual section IDs\n",
    "date_range = {\n",
    "    'start_date': '2024-01-01',\n",
    "    'end_date': '2024-12-31'\n",
    "}\n",
    "\n",
    "print(f\"Processing {len(section_ids)} sections...\")\n",
    "\n",
    "results = []\n",
    "for i, section_id in enumerate(section_ids, 1):\n",
    "    payload = {\n",
    "        'mode': 'bulk',\n",
    "        'section_id': section_id,\n",
    "        **date_range,\n",
    "        'force_reprocess': False\n",
    "    }\n",
    "\n",
    "    print(f\"  {i}/{len(section_ids)}: Triggering processing for section {section_id}...\")\n",
    "\n",
    "    result = invoke_lambda_async(payload)\n",
    "    results.append({\n",
    "        'section_id': section_id,\n",
    "        'success': result['success'],\n",
    "        'request_id': result.get('request_id'),\n",
    "        'error': result.get('error')\n",
    "    })\n",
    "\n",
    "    # Small delay to avoid overwhelming Lambda\n",
    "    time.sleep(1)\n",
    "\n",
    "# Summary\n",
    "successful = sum(1 for r in results if r['success'])\n",
    "failed = len(results) - successful\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  ‚úÖ Successfully triggered: {successful}\")\n",
    "print(f\"  ‚ùå Failed: {failed}\")\n",
    "\n",
    "if failed > 0:\n",
    "    print(\"\\nFailed sections:\")\n",
    "    for result in results:\n",
    "        if not result['success']:\n",
    "            print(f\"  - Section {result['section_id']}: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def5280f",
   "metadata": {},
   "source": [
    "## 5. Process All Available Data\n",
    "\n",
    "Process all available XAPI data (use with caution for large datasets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17967beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è WARNING: This will process ALL available data. Use carefully!\n",
    "process_all = False  # Set to True to enable\n",
    "\n",
    "if process_all:\n",
    "    all_data_payload = {\n",
    "        'mode': 'bulk',\n",
    "        's3_prefix': 'section/',\n",
    "        'start_date': '2024-01-01',  # Adjust as needed\n",
    "        'end_date': '2024-12-31',    # Adjust as needed\n",
    "        'force_reprocess': False\n",
    "    }\n",
    "\n",
    "    print(\"‚ö†Ô∏è  Processing ALL available data...\")\n",
    "    print(\"This is an asynchronous operation that may take a long time.\")\n",
    "\n",
    "    all_result = invoke_lambda_async(all_data_payload)\n",
    "\n",
    "    if all_result['success']:\n",
    "        print(f\"‚úÖ Successfully triggered bulk processing\")\n",
    "        print(f\"Request ID: {all_result['request_id']}\")\n",
    "        print(\"Monitor CloudWatch logs for progress.\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to trigger bulk processing: {all_result.get('error')}\")\n",
    "else:\n",
    "    print(\"Bulk processing disabled. Set process_all = True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec09df",
   "metadata": {},
   "source": [
    "## 6. Test Single File Processing\n",
    "\n",
    "Test processing of a single JSONL file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a specific file\n",
    "test_bucket = 'your-xapi-bucket'  # Replace with your S3 bucket\n",
    "test_key = 'section/123/video/2024-01-01T12-00-00.000Z_test-bundle.jsonl'  # Replace with actual file\n",
    "\n",
    "test_payload = {\n",
    "    'bucket': test_bucket,\n",
    "    'key': test_key\n",
    "}\n",
    "\n",
    "print(f\"Testing single file processing: s3://{test_bucket}/{test_key}\")\n",
    "\n",
    "test_result = invoke_lambda_sync(test_payload)\n",
    "\n",
    "if test_result['success']:\n",
    "    result_body = test_result['result']['body']\n",
    "    if isinstance(result_body, str):\n",
    "        result_data = json.loads(result_body)\n",
    "    else:\n",
    "        result_data = result_body\n",
    "\n",
    "    print(\"‚úÖ Single file processing completed\")\n",
    "    print(json.dumps(result_data, indent=2))\n",
    "else:\n",
    "    print(f\"‚ùå Single file processing failed: {test_result.get('error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d24d0",
   "metadata": {},
   "source": [
    "## 7. Monitoring and Troubleshooting\n",
    "\n",
    "Check Lambda function logs and status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check recent Lambda invocations (requires CloudWatch Logs access)\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "logs_client = boto3.client('logs', region_name=AWS_REGION)\n",
    "\n",
    "def get_recent_lambda_logs(function_name, hours=1):\n",
    "    \"\"\"Get recent logs for a Lambda function\"\"\"\n",
    "    log_group = f'/aws/lambda/{function_name}'\n",
    "\n",
    "    try:\n",
    "        end_time = datetime.utcnow()\n",
    "        start_time = end_time - timedelta(hours=hours)\n",
    "\n",
    "        response = logs_client.filter_log_events(\n",
    "            logGroupName=log_group,\n",
    "            startTime=int(start_time.timestamp() * 1000),\n",
    "            endTime=int(end_time.timestamp() * 1000),\n",
    "            limit=100\n",
    "        )\n",
    "\n",
    "        return response.get('events', [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting logs for {function_name}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Get recent logs for XAPI ETL processor\n",
    "print(f\"Recent logs for {XAPI_ETL_FUNCTION}:\")\n",
    "etl_logs = get_recent_lambda_logs(XAPI_ETL_FUNCTION)\n",
    "for event in etl_logs[-5:]:  # Show last 5 log events\n",
    "    timestamp = datetime.fromtimestamp(event['timestamp'] / 1000)\n",
    "    print(f\"[{timestamp}] {event['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e36ff",
   "metadata": {},
   "source": [
    "## 8. ClickHouse Data Verification\n",
    "\n",
    "If you have direct access to ClickHouse, you can verify the data was loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This requires direct ClickHouse access or a separate verification Lambda\n",
    "# Here's sample code for direct verification (uncomment and modify as needed)\n",
    "\n",
    "\"\"\"\n",
    "import requests\n",
    "\n",
    "# ClickHouse connection details\n",
    "CLICKHOUSE_HOST = 'your-clickhouse-host'\n",
    "CLICKHOUSE_PORT = 8123\n",
    "CLICKHOUSE_USER = 'default'\n",
    "CLICKHOUSE_PASSWORD = 'your-password'\n",
    "CLICKHOUSE_DATABASE = 'default'\n",
    "\n",
    "def query_clickhouse(query):\n",
    "    url = f\"http://{CLICKHOUSE_HOST}:{CLICKHOUSE_PORT}\"\n",
    "    headers = {\n",
    "        'Content-Type': 'text/plain',\n",
    "        'X-ClickHouse-User': CLICKHOUSE_USER,\n",
    "        'X-ClickHouse-Key': CLICKHOUSE_PASSWORD\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, data=query, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\"\n",
    "\n",
    "# Check total event count\n",
    "total_events_query = f\"SELECT COUNT(*) FROM {CLICKHOUSE_DATABASE}.video_events\"\n",
    "total_events = query_clickhouse(total_events_query)\n",
    "print(f\"Total video events in ClickHouse: {total_events}\")\n",
    "\n",
    "# Check events by section\n",
    "section_query = f\"\"\"\n",
    "SELECT section_id, COUNT(*) as event_count\n",
    "FROM {CLICKHOUSE_DATABASE}.video_events\n",
    "GROUP BY section_id\n",
    "ORDER BY event_count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "section_stats = query_clickhouse(section_query)\n",
    "print(f\"\\nEvents by section (top 10):\\n{section_stats}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"ClickHouse verification code provided above (uncomment and modify as needed)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
