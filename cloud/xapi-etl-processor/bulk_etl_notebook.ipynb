{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21b93cb",
   "metadata": {},
   "source": [
    "# OLI Torus XAPI ETL Pipeline - XAPI Events Processing\n",
    "\n",
    "This notebook provides an interface for bulk processing of historical XAPI data using AWS Lambda.\n",
    "\n",
    "### Supported Event Types:\n",
    "- `video` - Video interactions (play, pause, seek, complete)\n",
    "- `activity_attempt` - Activity completion events  \n",
    "- `page_attempt` - Page completion events\n",
    "- `page_viewed` - Page view tracking\n",
    "- `part_attempt` - Individual question responses\n",
    "\n",
    "## S3 ETL Performance\n",
    "\n",
    "The ETL pipeline uses **pure ClickHouse S3 ingestion** for all operations:\n",
    "\n",
    "- **ClickHouse S3 Ingestion**: Direct processing using ClickHouse's native `s3()` table functions\n",
    "- **Unified Processing**: Single high-performance SQL query for all event types\n",
    "- **No Timeout Limits**: Bypasses Lambda's 15-minute limitation completely\n",
    "- **Consistent Performance**: Same fast processing for any dataset size\n",
    "- **Cost Effective**: Minimal Lambda execution time - ClickHouse does the heavy lifting\n",
    "\n",
    "## Performance Benefits\n",
    "\n",
    "- **10x Faster Processing**: ClickHouse S3 integration + unified table provides significant performance improvements\n",
    "- **No Lambda Timeout Limits**: Process unlimited datasets without time constraints\n",
    "- **Simplified Queries**: Single table queries are much faster than multi-table JOINs\n",
    "- **Better Indexing**: LowCardinality event_type field enables efficient filtering and grouping\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **AWS Configuration**: AWS credentials with Lambda invoke and S3 access permissions\n",
    "2. **ClickHouse Access**: ClickHouse instance with S3 access and `raw_events` table created\n",
    "3. **Lambda Deployment**: Updated Lambda functions deployed with unified table support\n",
    "4. **Environment Setup**: Proper .env configuration for AWS and ClickHouse credentials\n",
    "\n",
    "## Compatibility\n",
    "\n",
    "- âœ… **Google Colab**: All dependencies installable via pip, no local file dependencies\n",
    "- âœ… **Local Jupyter**: Full functionality including direct local execution mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7bc98",
   "metadata": {},
   "source": [
    "## Quick Setup - Install Requirements\n",
    "\n",
    "Run the cell below to install all required dependencies for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc07367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required dependencies\n",
    "# Choose one of the options below:\n",
    "\n",
    "# Option 1: Install notebook-specific requirements (includes pandas and visualization tools)\n",
    "!pip install -r notebook-requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae94f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Environment Variables using python-dotenv\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Install python-dotenv if not already installed\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"ğŸ”§ Loading environment variables from .env file...\")\n",
    "\n",
    "# Load .env file - this is much simpler than manual parsing!\n",
    "if load_dotenv('.env'):\n",
    "    print(\"âœ… Environment variables loaded successfully!\")\n",
    "\n",
    "    # Check AWS configuration\n",
    "    AWS_REGION = os.getenv('AWS_REGION', 'us-east-1')\n",
    "    AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "    AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "    S3_XAPI_BUCKET = os.getenv('S3_XAPI_BUCKET')\n",
    "\n",
    "    print(f\"\\nğŸ” AWS Configuration Status:\")\n",
    "    print(f\"  AWS_ACCESS_KEY_ID: {'âœ… Set' if AWS_ACCESS_KEY_ID else 'âŒ Missing'}\")\n",
    "    print(f\"  AWS_SECRET_ACCESS_KEY: {'âœ… Set' if AWS_SECRET_ACCESS_KEY else 'âŒ Missing'}\")\n",
    "    print(f\"  AWS_REGION: {AWS_REGION if AWS_REGION else 'âŒ Missing'}\")\n",
    "    print(f\"  S3_XAPI_BUCKET: {S3_XAPI_BUCKET if S3_XAPI_BUCKET else 'âŒ Missing'}\")\n",
    "\n",
    "    # Initialize AWS clients\n",
    "    lambda_client = boto3.client('lambda', region_name=AWS_REGION)\n",
    "    s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "    ENVIRONMENT = os.getenv('ENVIRONMENT', 'dev')\n",
    "\n",
    "    # Unified Lambda function name (adjust based on your deployment)\n",
    "    XAPI_ETL_FUNCTION = f'xapi-etl-processor-{ENVIRONMENT}'\n",
    "\n",
    "    print(f\"Configured for {ENVIRONMENT} environment in {AWS_REGION}\")\n",
    "    print(f\"Unified XAPI ETL Function: {XAPI_ETL_FUNCTION}\")\n",
    "\n",
    "    EXECUTION_MODE = os.getenv('EXECUTION_MODE', 'local').lower()\n",
    "\n",
    "    # For local execution, we'll import the required modules\n",
    "    if EXECUTION_MODE == 'local':\n",
    "        import sys\n",
    "        import os\n",
    "\n",
    "        # Add current directory to path to import local modules\n",
    "        current_dir = os.path.dirname(os.path.abspath('__file__' if '__file__' in globals() else os.getcwd()))\n",
    "        if current_dir not in sys.path:\n",
    "            sys.path.insert(0, current_dir)\n",
    "\n",
    "        # Import local modules (using unified versions)\n",
    "        try:\n",
    "            from lambda_function import lambda_handler, health_check\n",
    "            from common import get_config\n",
    "            from clickhouse_client import ClickHouseClient\n",
    "            print(f\"âœ… Local unified modules imported successfully\")\n",
    "            print(f\"   - Using unified raw_events table model\")\n",
    "            print(f\"   - All event types processed into single table\")\n",
    "        except ImportError as e:\n",
    "            print(f\"âŒ Failed to import local modules: {e}\")\n",
    "            print(\"Make sure you're running this notebook from the xapi-etl-processor directory\")\n",
    "            print(\"Exiting due to failed local module imports.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    print(f\"ğŸ”§ Execution mode: {EXECUTION_MODE.upper()}\")\n",
    "    if EXECUTION_MODE == 'lambda':\n",
    "        print(f\"   Using Lambda function: {XAPI_ETL_FUNCTION}\")\n",
    "        print(f\"   ğŸ“Š Lambda processes via unified raw_events table\")\n",
    "    else:\n",
    "        print(f\"   Running locally with unified table model\")\n",
    "        print(f\"   ğŸ“Š Direct ClickHouse processing into raw_events table\")\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ Could not load .env file\")\n",
    "    print(f\"   Make sure .env file exists with your AWS credentials:\")\n",
    "    print(f\"   cp example.env .env\")\n",
    "    print(f\"   # Then edit .env with your actual credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b06a0",
   "metadata": {},
   "source": [
    "## Execution Modes\n",
    "\n",
    "This notebook supports two execution modes:\n",
    "\n",
    "### Local Mode (Default)\n",
    "- **EXECUTION_MODE = 'local'**\n",
    "- Runs the Lambda function code locally in this notebook\n",
    "- Requires local modules (lambda_function.py, common.py, clickhouse_client.py)\n",
    "- Good for testing and development\n",
    "- All operations run synchronously in the notebook\n",
    "\n",
    "### Lambda Mode\n",
    "- **EXECUTION_MODE = 'lambda'**\n",
    "- Invokes the deployed AWS Lambda function remotely\n",
    "- Requires AWS credentials and deployed Lambda function\n",
    "- Best for production use and processing large datasets\n",
    "- Supports true asynchronous processing\n",
    "\n",
    "**To switch modes:** Change the `EXECUTION_MODE` variable in the cell above and re-run that cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd690b",
   "metadata": {},
   "source": [
    "## Clean API Architecture\n",
    "\n",
    "This notebook uses a **clean, optimized API** with **unified section processing**:\n",
    "\n",
    "- **All processing logic** resides in the `lambda_function.py`\n",
    "- **Notebook is just a caller** - no complex processing logic here\n",
    "- **Unified section processing** - single or multiple sections use the same code path\n",
    "- **Clean API** - only `section_ids` parameter (always a list)\n",
    "- **Consistent interface** regardless of local or Lambda execution mode\n",
    "- **Lambda function handles**:\n",
    "  - Single file processing (S3 event triggers)\n",
    "  - Unified bulk processing (`section_ids: [id]` for single, `section_ids: [id1, id2, ...]` for multiple)\n",
    "  - Automatic batching for multiple sections\n",
    "  - Error handling and retries\n",
    "  - Progress tracking and reporting\n",
    "\n",
    "### Benefits of This Approach\n",
    "\n",
    "1. **Single Source of Truth**: All logic in `lambda_function.py`\n",
    "2. **Unified Code Path**: Single or multiple sections use identical processing logic\n",
    "3. **Clean API**: No deprecated parameters, consistent interface\n",
    "4. **Easy Testing**: Test locally before deploying to Lambda\n",
    "5. **Consistent Behavior**: Same results whether run locally or remotely\n",
    "6. **Simplified Debugging**: All processing logic in one place\n",
    "7. **Easy Deployment**: Just deploy the Lambda function, notebook stays simple\n",
    "8. **Flexible Scaling**: Single section is just a list with one element\n",
    "\n",
    "### API Design\n",
    "\n",
    "- **Single Section**: `section_ids: [123]` \n",
    "- **Multiple Sections**: `section_ids: [123, 124, 125]`\n",
    "- **Required Parameters**: `mode: \"bulk\"`, `section_ids: [...]`\n",
    "- **Optional Parameters**: `batch_size`, `force_reprocess`, `dry_run`, `start_date`, `end_date`, `s3_bucket`, `s3_prefix`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265f6a9",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae00b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_function(payload):\n",
    "    \"\"\"Execute function either locally or via Lambda based on EXECUTION_MODE\"\"\"\n",
    "    if EXECUTION_MODE == 'local':\n",
    "        return execute_local(payload)\n",
    "    else:\n",
    "        return invoke_lambda_sync(payload)\n",
    "\n",
    "def execute_function_async(payload):\n",
    "    \"\"\"Execute function asynchronously - Lambda only or simulate locally\"\"\"\n",
    "    if EXECUTION_MODE == 'local':\n",
    "        # For local execution, run synchronously but indicate it's \"async\"\n",
    "        result = execute_local(payload)\n",
    "        if result['success']:\n",
    "            result['async_mode'] = True\n",
    "            result['request_id'] = f\"local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "        return result\n",
    "    else:\n",
    "        return invoke_lambda_async(payload)\n",
    "\n",
    "def execute_local(payload):\n",
    "    \"\"\"Execute the Lambda function locally\"\"\"\n",
    "    try:\n",
    "        # Create a mock context object\n",
    "        class MockContext:\n",
    "            def __init__(self):\n",
    "                self.function_name = \"xapi-etl-processor-local\"\n",
    "                self.function_version = \"1\"\n",
    "                self.aws_request_id = f\"local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "        context = MockContext()\n",
    "\n",
    "        # Call the lambda_handler function directly\n",
    "        result = lambda_handler(payload, context)\n",
    "\n",
    "        return {\n",
    "            'success': True,\n",
    "            'status_code': result.get('statusCode', 200),\n",
    "            'result': result,\n",
    "            'execution_mode': 'local'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'execution_mode': 'local'\n",
    "        }\n",
    "\n",
    "def invoke_lambda_async(payload):\n",
    "    \"\"\"Invoke the Lambda function asynchronously\"\"\"\n",
    "    try:\n",
    "        response = lambda_client.invoke(\n",
    "            FunctionName=XAPI_ETL_FUNCTION,\n",
    "            InvocationType='Event',  # Async invocation\n",
    "            Payload=json.dumps(payload)\n",
    "        )\n",
    "        return {\n",
    "            'success': True,\n",
    "            'status_code': response['StatusCode'],\n",
    "            'request_id': response['ResponseMetadata']['RequestId'],\n",
    "            'execution_mode': 'lambda',\n",
    "            'async_mode': True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'execution_mode': 'lambda'\n",
    "        }\n",
    "\n",
    "def invoke_lambda_sync(payload):\n",
    "    \"\"\"Invoke the Lambda function synchronously\"\"\"\n",
    "    try:\n",
    "        response = lambda_client.invoke(\n",
    "            FunctionName=XAPI_ETL_FUNCTION,\n",
    "            InvocationType='RequestResponse',  # Sync invocation\n",
    "            Payload=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "        result = json.loads(response['Payload'].read().decode())\n",
    "        return {\n",
    "            'success': True,\n",
    "            'status_code': response['StatusCode'],\n",
    "            'result': result,\n",
    "            'execution_mode': 'lambda'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'execution_mode': 'lambda'\n",
    "        }\n",
    "\n",
    "def check_function_health():\n",
    "    \"\"\"Check if the function is healthy (Lambda or local)\"\"\"\n",
    "    payload = {'health_check': True}\n",
    "    return execute_function(payload)\n",
    "\n",
    "print(\"âœ… Helper functions loaded\")\n",
    "print(f\"ğŸ”§ Execution mode: {EXECUTION_MODE}\")\n",
    "print(f\"ğŸ¯ All processing logic now handled by Lambda function\")\n",
    "print(f\"ğŸ“ Notebook acts as a simple caller interface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd18091",
   "metadata": {},
   "source": [
    "## 1. Health Checks\n",
    "\n",
    "First, let's verify that our Lambda functions and ClickHouse are healthy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7014ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check XAPI ETL processor health\n",
    "print(\"Checking XAPI ETL processor health...\")\n",
    "health = check_function_health()\n",
    "print(json.dumps(health, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40619cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clear existing section data before reprocessing\n",
    "# Uncomment and run this cell if you want to start fresh for this section\n",
    "\n",
    "clear_section_data = False  # Set to True to enable clearing\n",
    "target_section_id = '145'   # Make sure this matches your section ID above\n",
    "\n",
    "if clear_section_data:\n",
    "    print(f\"âš ï¸  WARNING: This will delete ALL existing data for section {target_section_id}\")\n",
    "    print(f\"   ğŸ—ƒï¸  From unified raw_events table (all event types)\")\n",
    "\n",
    "    if EXECUTION_MODE == 'local':\n",
    "        try:\n",
    "            # For local execution, directly use ClickHouse client\n",
    "            from clickhouse_client import ClickHouseClient\n",
    "            clickhouse_client = ClickHouseClient()\n",
    "\n",
    "            # Delete all events for this section from the unified table\n",
    "            print(f\"ğŸ§¹ Clearing unified raw_events table for section {target_section_id}...\")\n",
    "\n",
    "            try:\n",
    "                # Get current count before deletion\n",
    "                current_count = clickhouse_client.get_section_event_count(int(target_section_id))\n",
    "                print(f\"   ğŸ“Š Current events in section: {current_count}\")\n",
    "\n",
    "                if current_count > 0:\n",
    "                    # Delete all events for this section\n",
    "                    result = clickhouse_client.delete_section_events(int(target_section_id))\n",
    "                    print(f\"âœ… Cleared all events for section {target_section_id} from raw_events table\")\n",
    "\n",
    "                    # Verify deletion\n",
    "                    remaining_count = clickhouse_client.get_section_event_count(int(target_section_id))\n",
    "                    print(f\"   ğŸ“Š Remaining events: {remaining_count}\")\n",
    "                else:\n",
    "                    print(f\"   ğŸ’¡ No existing events found for section {target_section_id}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Error clearing section data: {str(e)}\")\n",
    "\n",
    "            print(f\"âœ… Section {target_section_id} data cleared - you can now set force_reprocess=False\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to clear section data: {str(e)}\")\n",
    "    else:\n",
    "        print(\"âŒ Data clearing only available in local mode\")\n",
    "        print(\"   For Lambda mode, use force_reprocess=True instead\")\n",
    "        print(\"   ğŸ’¡ The unified table approach makes data management more efficient\")\n",
    "else:\n",
    "    print(\"ğŸ’¡ Section data clearing disabled\")\n",
    "    print(\"   Set clear_section_data = True above to enable section data clearing\")\n",
    "    print(\"   ğŸ—ƒï¸  Unified table: All event types stored in single raw_events table\")\n",
    "    print(\"   ğŸ’¡ Alternatively, use force_reprocess=True in processing calls to overwrite existing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff9424",
   "metadata": {},
   "source": [
    "## 3. Process Single Section\n",
    "\n",
    "Process data for a single section using the unified bulk processing approach. Single sections are handled as a list with one element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a single section using the Lambda function (unified approach)\n",
    "target_section_id = 145  # Change this to your section ID\n",
    "force_reprocess_single = False  # Set to True to reprocess existing data\n",
    "\n",
    "print(f\"ğŸ¯ Processing single section: {target_section_id}\")\n",
    "print(f\"ğŸ—ƒï¸  Target: Unified raw_events table\")\n",
    "\n",
    "# Create payload for single section processing (using unified approach)\n",
    "single_section_payload = {\n",
    "    'mode': 'bulk',\n",
    "    'section_ids': [target_section_id],  # Single section as list\n",
    "    'force_reprocess': force_reprocess_single,\n",
    "    's3_bucket': S3_XAPI_BUCKET,\n",
    "    's3_prefix': 'section/'\n",
    "}\n",
    "\n",
    "print(f\"ğŸš€ Starting processing for section {target_section_id}...\")\n",
    "\n",
    "# Execute the function (either locally or via Lambda)\n",
    "result = execute_function(single_section_payload)\n",
    "\n",
    "if result['success']:\n",
    "    result_body = result['result']['body']\n",
    "    if isinstance(result_body, str):\n",
    "        result_data = json.loads(result_body)\n",
    "    else:\n",
    "        result_data = result_body\n",
    "\n",
    "    print(\"âœ… Single section processing completed\")\n",
    "\n",
    "    # Display results\n",
    "    processing_mode = result_data.get('processing_mode', 'unknown')\n",
    "    total_events = result_data.get('total_events_processed', 0)\n",
    "    files_found = result_data.get('files_found', 0)\n",
    "    processing_method = result_data.get('processing_method', 'unknown')\n",
    "\n",
    "    print(f\"\\nğŸ“Š Processing Results:\")\n",
    "    print(f\"   Processing mode: {processing_mode}\")\n",
    "    print(f\"   ğŸ“ Files found: {files_found}\")\n",
    "    print(f\"   ğŸ“ˆ Total events processed: {total_events}\")\n",
    "    print(f\"   ğŸ”§ Processing method: {processing_method}\")\n",
    "\n",
    "    # Show per-section details\n",
    "    section_results = result_data.get('section_results', {})\n",
    "    section_result = section_results.get(str(target_section_id), {})\n",
    "\n",
    "    if section_result:\n",
    "        status = section_result.get('status', 'unknown')\n",
    "        files_processed = section_result.get('files_processed', 0)\n",
    "\n",
    "        if status == 'success':\n",
    "            print(f\"   âœ… Section status: {status}\")\n",
    "            print(f\"   ğŸ“„ Files processed: {files_processed}\")\n",
    "        elif status == 'skipped':\n",
    "            reason = section_result.get('reason', 'Unknown reason')\n",
    "            print(f\"   â­ï¸  Section status: {status} - {reason}\")\n",
    "        elif status == 'failed':\n",
    "            error = section_result.get('error', 'Unknown error')\n",
    "            print(f\"   âŒ Section status: {status} - {error}\")\n",
    "\n",
    "    print(f\"\\nğŸƒ Execution mode: {result.get('execution_mode', 'unknown')}\")\n",
    "\n",
    "    if EXECUTION_MODE == 'lambda' and 'request_id' in result:\n",
    "        print(f\"ğŸ“‹ Request ID: {result['request_id']}\")\n",
    "\n",
    "    # Check if processing was skipped due to existing data\n",
    "    if 'message' in result_data and 'already exists' in result_data['message']:\n",
    "        existing_count = result_data.get('existing_events_count', 0)\n",
    "        print(f\"\\nğŸ’¡ Processing was skipped - section already has {existing_count} events\")\n",
    "        print(f\"   Set force_reprocess_single = True to reprocess anyway\")\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ Single section processing failed: {result.get('error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de0658",
   "metadata": {},
   "source": [
    "## 4. Process Multiple Sections\n",
    "\n",
    "Process data for multiple sections using the unified bulk processing approach. The same Lambda function handles both single and multiple sections seamlessly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple sections using the Lambda function (unified approach)\n",
    "sections_to_process = [145]  # Add your section IDs here\n",
    "batch_size = 5  # Process sections in batches to avoid timeouts\n",
    "force_reprocess_batch = False  # Set to True to reprocess existing data\n",
    "\n",
    "print(f\"ğŸ”„ Processing {len(sections_to_process)} sections with unified approach\")\n",
    "print(f\"ğŸ“¦ Batch size: {batch_size} sections per batch\")\n",
    "print(f\"ğŸ—ƒï¸  Target: Unified raw_events table\")\n",
    "\n",
    "if sections_to_process:\n",
    "    # Create payload for multiple sections processing (unified approach)\n",
    "    payload = {\n",
    "        'mode': 'bulk',\n",
    "        'section_ids': sections_to_process,  # Works for single or multiple sections\n",
    "        'batch_size': batch_size,\n",
    "        'force_reprocess': force_reprocess_batch,\n",
    "        's3_bucket': S3_XAPI_BUCKET,\n",
    "        's3_prefix': 'section/'\n",
    "    }\n",
    "\n",
    "    print(f\"ğŸ¯ Processing {len(sections_to_process)} sections...\")\n",
    "\n",
    "    # Execute the function (either locally or via Lambda)\n",
    "    result = execute_function(payload)\n",
    "\n",
    "    if result['success']:\n",
    "        result_body = result['result']['body']\n",
    "        if isinstance(result_body, str):\n",
    "            result_data = json.loads(result_body)\n",
    "        else:\n",
    "            result_data = result_body\n",
    "\n",
    "        print(\"âœ… Sections processing completed\")\n",
    "\n",
    "        # Display summary\n",
    "        processing_mode = result_data.get('processing_mode', 'unknown')\n",
    "        total_sections = result_data.get('total_sections_requested', 0)\n",
    "        successful_sections = result_data.get('successful_sections', 0)\n",
    "        failed_sections = result_data.get('failed_sections', 0)\n",
    "        total_events = result_data.get('total_events_processed', 0)\n",
    "\n",
    "        print(f\"\\nğŸ“Š Processing Summary:\")\n",
    "        print(f\"   ğŸ”§ Processing mode: {processing_mode}\")\n",
    "        print(f\"   ğŸ“‹ Total sections requested: {total_sections}\")\n",
    "        print(f\"   âœ… Successful: {successful_sections} sections\")\n",
    "        print(f\"   âŒ Failed: {failed_sections} sections\")\n",
    "        print(f\"   ğŸ“ˆ Total events processed: {total_events}\")\n",
    "\n",
    "        # Show successful section IDs\n",
    "        if 'successful_section_ids' in result_data:\n",
    "            print(f\"   ğŸ¯ Successful sections: {result_data['successful_section_ids']}\")\n",
    "\n",
    "        # Show failed section IDs\n",
    "        if 'failed_section_ids' in result_data:\n",
    "            print(f\"   âš ï¸  Failed sections: {result_data['failed_section_ids']}\")\n",
    "\n",
    "        # Show per-section details\n",
    "        section_results = result_data.get('section_results', {})\n",
    "        if section_results:\n",
    "            print(f\"\\nğŸ“ Per-Section Details:\")\n",
    "            for section_id, section_result in section_results.items():\n",
    "                status = section_result.get('status', 'unknown')\n",
    "                events = section_result.get('total_events_processed', 0)\n",
    "                files_processed = section_result.get('files_processed', 0)\n",
    "                files_found = section_result.get('files_found', 0)\n",
    "\n",
    "                if status == 'success':\n",
    "                    print(f\"   Section {section_id}: âœ… {events} events processed ({files_processed} files)\")\n",
    "                elif status == 'failed':\n",
    "                    error = section_result.get('error', 'Unknown error')\n",
    "                    print(f\"   Section {section_id}: âŒ Failed - {error}\")\n",
    "                elif status == 'skipped':\n",
    "                    reason = section_result.get('reason', 'Unknown reason')\n",
    "                    print(f\"   Section {section_id}: â­ï¸  Skipped - {reason} ({files_found} files found)\")\n",
    "                elif status == 'dry_run':\n",
    "                    print(f\"   Section {section_id}: ğŸ” Dry run - {files_found} files found\")\n",
    "\n",
    "        print(f\"\\nğŸ”§ Processing method: {result_data.get('processing_method', 'unknown')}\")\n",
    "        print(f\"ğŸƒ Execution mode: {result.get('execution_mode', 'unknown')}\")\n",
    "\n",
    "        if EXECUTION_MODE == 'lambda' and 'request_id' in result:\n",
    "            print(f\"ğŸ“‹ Request ID: {result['request_id']}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"âŒ Sections processing failed: {result.get('error')}\")\n",
    "\n",
    "else:\n",
    "    print(\"ğŸ’¡ No sections specified for processing\")\n",
    "    print(\"   Add section IDs to sections_to_process list above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec09df",
   "metadata": {},
   "source": [
    "## 5. Test Single File Processing\n",
    "\n",
    "Test processing of a single JSONL file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a specific file using the Lambda function\n",
    "test_bucket = S3_XAPI_BUCKET  # Use the configured bucket\n",
    "test_key = 'section/145/video/2024-01-01T12-00-00.000Z_test-bundle.jsonl'  # Replace with actual file\n",
    "\n",
    "print(f\"ğŸ” Testing single file processing...\")\n",
    "print(f\"ğŸ“ File: s3://{test_bucket}/{test_key}\")\n",
    "\n",
    "# Create test payload for single file processing\n",
    "test_payload = {\n",
    "    'bucket': test_bucket,\n",
    "    'key': test_key\n",
    "}\n",
    "\n",
    "# Execute the function (either locally or via Lambda)\n",
    "test_result = execute_function(test_payload)\n",
    "\n",
    "if test_result['success']:\n",
    "    result_body = test_result['result']['body']\n",
    "    if isinstance(result_body, str):\n",
    "        result_data = json.loads(result_body)\n",
    "    else:\n",
    "        result_data = result_body\n",
    "\n",
    "    print(\"âœ… Single file processing completed\")\n",
    "\n",
    "    # Display results\n",
    "    events_processed = result_data.get('total_events_processed', 0)\n",
    "    section_id = result_data.get('section_id', 'unknown')\n",
    "    processing_method = result_data.get('processing_method', 'unknown')\n",
    "\n",
    "    print(f\"\\nğŸ“Š Processing Results:\")\n",
    "    print(f\"   ğŸ“„ Section ID: {section_id}\")\n",
    "    print(f\"   ğŸ“ˆ Events processed: {events_processed}\")\n",
    "    print(f\"   ğŸ”§ Processing method: {processing_method}\")\n",
    "    print(f\"   ğŸƒ Execution mode: {test_result.get('execution_mode', 'unknown')}\")\n",
    "\n",
    "    if EXECUTION_MODE == 'lambda' and 'request_id' in test_result:\n",
    "        print(f\"   ğŸ“‹ Request ID: {test_result['request_id']}\")\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ Single file processing failed: {test_result.get('error')}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Note: Make sure the test file exists in S3 before running this test\")\n",
    "print(f\"   You can use AWS CLI or S3 console to verify: aws s3 ls s3://{test_bucket}/{test_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d24d0",
   "metadata": {},
   "source": [
    "## 6. Monitoring and Troubleshooting\n",
    "\n",
    "Check Lambda function logs and status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check recent Lambda invocations (requires CloudWatch Logs access)\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "logs_client = boto3.client('logs', region_name=AWS_REGION)\n",
    "\n",
    "def get_recent_lambda_logs(function_name, hours=1):\n",
    "    \"\"\"Get recent logs for a Lambda function\"\"\"\n",
    "    log_group = f'/aws/lambda/{function_name}'\n",
    "\n",
    "    try:\n",
    "        end_time = datetime.utcnow()\n",
    "        start_time = end_time - timedelta(hours=hours)\n",
    "\n",
    "        response = logs_client.filter_log_events(\n",
    "            logGroupName=log_group,\n",
    "            startTime=int(start_time.timestamp() * 1000),\n",
    "            endTime=int(end_time.timestamp() * 1000),\n",
    "            limit=100\n",
    "        )\n",
    "\n",
    "        return response.get('events', [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting logs for {function_name}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Get recent logs for XAPI ETL processor\n",
    "print(f\"Recent logs for {XAPI_ETL_FUNCTION}:\")\n",
    "etl_logs = get_recent_lambda_logs(XAPI_ETL_FUNCTION)\n",
    "for event in etl_logs[-5:]:  # Show last 5 log events\n",
    "    timestamp = datetime.fromtimestamp(event['timestamp'] / 1000)\n",
    "    print(f\"[{timestamp}] {event['message']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
