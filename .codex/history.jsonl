{"session_id":"a427b74b-b18e-4b49-8c1b-4df193b4d6e0","ts":1757699390,"text":"---\ndescription: Designs a new Torus feature based on an informal description or .prd file\n---\n\n## Inputs\n- Either the content of an informally described feature OR a file path to a formal Product Requirements Document (.prd.md) will be present in @$ARGUMENTS.\n\n## Task\n\nYou are an Elixir / Phoenix architect who specializes in providing your expertise to guide others on how to best design a new feature in the Torus codebase.\n\n## Focus Areas\n\n- Context capture: Summarize the relevant Torus architecture (domains, contexts, data model, runtime topology, deployment, tenancy).\n- OTP alignment: Apply OTP design principles (supervision, process lifecycles, failure isolation, backpressure, message passing, state ownership).\n- Performance & scalability: Query efficiency, indexing, connection pools, caches (ETS/persistent_term/SectionResourceDepot), PubSub, horizontal scale, memory/GC, mailbox growth.\n- Data model & consistency: Ecto schemas/migrations, constraints, read/write paths, transaction boundaries, eventual vs. strong consistency, idempotency.\n- Concurrency patterns: GenServer/Task/Registry/Partitioned ETS, GenStage/Broadway where applicable, avoiding bottleneck processes, sharding keys.\n- Caching strategy: Layers, TTL/invalidation, cache stamps, write-through/around, fanout containment, cold-start behavior, multi-node coherence.\n- Observability: :telemetry events, OpenTelemetry/AppSignal, logging and log structure, alerts & SLOs.\n- Security & privacy: AuthN/AuthZ, tenant isolation, PII handling, audit trails, least privilege, secure defaults.\n- Ops & rollout: Feature flags, config toggles, online migrations/backfills, canarying, rollback plan, disaster recovery.\n- Backwards compatibility (particularly for activity changes or page content model changes)\n- Developer ergonomics: Testability, clear module boundaries, documentation\n\n\n## Approach\n\n- Study the PRD or informal feature description\n- Ingest relevant Torus docs (local): Read and summarize ./guides/design/**/*.md (architecture, runtime, domain model, data flow, operational guides). Build a “What I know / Don’t know” list.\n- Codebase waypointing (lightweight): Map key modules & boundaries (contexts, schemas, LiveViews, background jobs, cache modules). Note entry points, supervision trees, and existing telemetry.\n- Requirements restatement: Rewrite the feature request in your own words. List explicit/non-goals, constraints, performanc expectations and success criteria.\n- Document Assumptions: If information is missing, write explicit assumptions. Proceed without blocking; call out risks created by each assumption.\n- Perform External research: Survey Elixir/Erlang/Phoenix best practices and patterns relevant to the feature (OTP design principles, Phoenix/LiveView patterns, Ecto performance, ETS/caching, clustering, GenStage/Broadway, PubSub). Prefer primary sources (official docs, hexdocs, José Valim posts, BEAM VM docs). Capture citations with titles + URLs + access dates.\n- Think hardest about the feature request at hand and iterate to produce an approach and documented design that meets the functional requirements and that addresses the non functional requirements\n\n## Output a Detailed Design\n\nOutput a Feature Design Document, using this Template as guidance.  Not all design tasks will\nneed all of the following sections.  Use your expert judgement on which sections to include, though\nthe first four below are absolutely required.\n\n\n  1. Executive Summary\n\n  Plain-English overview: what this delivers, who it affects, why this design, headline risks, and performance posture. (8–10 sentences)\n\n  2. Requirements & Assumptions\n\n  - Functional Requirements: Bulleted list of core functionality\n  - Non-Functional Requirements: Latency/throughput/SLOs targets\n  - Explicit Assumptions: Key assumptions with impact assessment\n\n  3. Torus Context Summary\n\n  What you learned from ./guides and code reconnaissance: modules, boundaries, current supervision, relevant telemetry hooks.\n\n  4. Proposed Design\n\n  4.1 Component Roles & Interactions\n\n  Component roles and interactions in prose (no diagrams).\n\n  4.2 State & Message Flow\n\n  Ownership of state, message flows, and backpressure points.\n\n  4.3 Supervision & Lifecycle\n\n  Tree placement, restart strategies, failure isolation.\n\n  4.4 Alternatives Consdiered\n\n  Briefly summarize any alternative approaches that were considered\n\n  5. Interfaces\n\n  5.1 HTTP/JSON APIs\n\n  Routes, params, validation, responses, rate limits.\n\n  5.2 LiveView\n\n  Events/handle_* callbacks, assigns touched, PubSub topics.\n\n  5.3 Processes\n\n  GenServer messages/callbacks, Registry keys, GenStage/Broadway pipelines.\n\n  6. Data Model & Storage\n\n  6.1 Ecto Schemas\n\n  Fields, constraints, online migrations plan, indexes (multi-column/partial/GIN) with rationale.\n\n  6.2 Query Performance\n\n  Representative queries with EXPLAIN (ANALYZE, BUFFERS) notes or expected plans.\n\n  7. Consistency & Transactions\n\n  Transaction boundaries, idempotency, retriable flows, compensation strategies.\n\n  8. Caching Strategy\n\n  What/where to cache (ETS/persistent_term/CacheEx/SectionResourceDepot), keys & partitioning, TTLs, invalidation triggers, multi-node\n  coherence.\n\n  9. Performance and Scalability Plan\n\n  9.1 Budgets\n\n  Target latencies (P50/P95/P99), max allocations/op, DB QPS, Repo pool sizing, ETS memory ceiling.\n\n  9.2 Load Tests\n\n  k6/wrk/bombardier scenarios (payloads, RPS ramp, think time), pass/fail gates, \"stop-the-line\" alerts.\n\n  9.3 Hotspots & Mitigations\n\n  N+1s, large payloads, mailbox growth, long-running sync work, lock contention, fan-out/fan-in.\n\n  10. Failure Modes & Resilience\n\n  Expected failures, timeouts/retries with jitter, circuit breakers, backoff policies, dead-letter handling, graceful shutdown.\n\n  11. Observability\n\n  Telemetry events (names, measurements, metadata), metric cardinality guardrails, traces to instrument, structured logs, alert\n  thresholds tied to SLOs.\n\n  12. Security & Privacy\n\n  AuthN/AuthZ concerns, PII handling/redaction, tenant isolation checks, audit events.\n\n  13. Testing Strategy\n\n  Unit/property tests, integration (Repo/Phoenix), concurrency/race tests, failure injection/chaos cases, migration/backfill\n  verification.\n\n  15. Risks & Mitigations\n\n  Top risks (technical, performance, data, operational) with concrete mitigations or fallbacks.\n\n  16. Open Questions & Follow-ups\n\n  Decisions needed from product/infrastructure teams with suggested defaults.\n\n  17. References\n\n  External research with Title · URL · Accessed date; prioritize official docs/hexdocs/erlang/phoenix sources.\n\n"}
{"session_id":"96c5211b-0129-4e5b-b4e0-276e469a7ba8","ts":1757700157,"text":"\n## Inputs\n- Ask user for a description of a feature that they want to build\n\n## Task\n- Output a poem about the feature.\n"}
{"session_id":"96c5211b-0129-4e5b-b4e0-276e469a7ba8","ts":1757700207,"text":"Caching of student responses for high throughput analytics display"}
{"session_id":"5b0839f8-b377-4ea0-970e-e7615a5a2851","ts":1757700425,"text":"\n## Inputs\n- Ask user for a description of a feature that they want to build or for the path for a filename to read which contains a product requirements document.\n\n## Task\n\nYou are an Elixir / Phoenix architect who specializes in providing your expertise to guide others on how to best design a new feature in the Torus codebase.\n\nRead in the PRD document or starting from the informal feature description,\nresearch and propose a sound technical approach for the feature.\n\n## Focus Areas\n\n- Context capture: Summarize the relevant Torus architecture (domains, contexts, data model, runtime topology, deployment, tenancy).\n- OTP alignment: Apply OTP design principles (supervision, process lifecycles, failure isolation, backpressure, message passing, state ownership).\n- Performance & scalability: Query efficiency, indexing, connection pools, caches (ETS/persistent_term/SectionResourceDepot), PubSub, horizontal scale, memory/GC, mailbox growth.\n- Data model & consistency: Ecto schemas/migrations, constraints, read/write paths, transaction boundaries, eventual vs. strong consistency, idempotency.\n- Concurrency patterns: GenServer/Task/Registry/Partitioned ETS, GenStage/Broadway where applicable, avoiding bottleneck processes, sharding keys.\n- Caching strategy: Layers, TTL/invalidation, cache stamps, write-through/around, fanout containment, cold-start behavior, multi-node coherence.\n- Observability: :telemetry events, OpenTelemetry/AppSignal, logging and log structure, alerts & SLOs.\n- Security & privacy: AuthN/AuthZ, tenant isolation, PII handling, audit trails, least privilege, secure defaults.\n- Ops & rollout: Feature flags, config toggles, online migrations/backfills, canarying, rollback plan, disaster recovery.\n- Backwards compatibility (particularly for activity changes or page content model changes)\n- Developer ergonomics: Testability, clear module boundaries, documentation\n\n\n## Approach\n\n- Study the PRD or informal feature description\n- Ingest relevant Torus docs (local): Read and summarize ./guides/design/**/*.md (architecture, runtime, domain model, data flow, operational guides). Build a “What I know / Don’t know” list.\n- Codebase waypointing (lightweight): Map key modules & boundaries (contexts, schemas, LiveViews, background jobs, cache modules). Note entry points, supervision trees, and existing telemetry.\n- Requirements restatement: Rewrite the feature request in your own words. List explicit/non-goals, constraints, performanc expectations and success criteria.\n- Document Assumptions: If information is missing, write explicit assumptions. Proceed without blocking; call out risks created by each assumption.\n- Perform External research: Survey Elixir/Erlang/Phoenix best practices and patterns relevant to the feature (OTP design principles, Phoenix/LiveView patterns, Ecto performance, ETS/caching, clustering, GenStage/Broadway, PubSub). Prefer primary sources (official docs, hexdocs, José Valim posts, BEAM VM docs). Capture citations with titles + URLs + access dates.\n- Think hardest about the feature request at hand and iterate to produce an approach and documented design that meets the functional requirements and that addresses the non functional requirements\n\n## Output a Detailed Design\n\nOutput a Feature Design Document, using this Template as guidance.  Not all design tasks will\nneed all of the following sections.  Use your expert judgement on which sections to include, though\nthe first four below are absolutely required.\n\n\n  1. Executive Summary\n\n  Plain-English overview: what this delivers, who it affects, why this design, headline risks, and performance posture. (8–10 sentences)\n\n  2. Requirements & Assumptions\n\n  - Functional Requirements: Bulleted list of core functionality\n  - Non-Functional Requirements: Latency/throughput/SLOs targets\n  - Explicit Assumptions: Key assumptions with impact assessment\n\n  3. Torus Context Summary\n\n  What you learned from ./guides and code reconnaissance: modules, boundaries, current supervision, relevant telemetry hooks.\n\n  4. Proposed Design\n\n  4.1 Component Roles & Interactions\n\n  Component roles and interactions in prose (no diagrams).\n\n  4.2 State & Message Flow\n\n  Ownership of state, message flows, and backpressure points.\n\n  4.3 Supervision & Lifecycle\n\n  Tree placement, restart strategies, failure isolation.\n\n  4.4 Alternatives Consdiered\n\n  Briefly summarize any alternative approaches that were considered\n\n  5. Interfaces\n\n  5.1 HTTP/JSON APIs\n\n  Routes, params, validation, responses, rate limits.\n\n  5.2 LiveView\n\n  Events/handle_* callbacks, assigns touched, PubSub topics.\n\n  5.3 Processes\n\n  GenServer messages/callbacks, Registry keys, GenStage/Broadway pipelines.\n\n  6. Data Model & Storage\n\n  6.1 Ecto Schemas\n\n  Fields, constraints, online migrations plan, indexes (multi-column/partial/GIN) with rationale.\n\n  6.2 Query Performance\n\n  Representative queries with EXPLAIN (ANALYZE, BUFFERS) notes or expected plans.\n\n  7. Consistency & Transactions\n\n  Transaction boundaries, idempotency, retriable flows, compensation strategies.\n\n  8. Caching Strategy\n\n  What/where to cache (ETS/persistent_term/CacheEx/SectionResourceDepot), keys & partitioning, TTLs, invalidation triggers, multi-node\n  coherence.\n\n  9. Performance and Scalability Plan\n\n  9.1 Budgets\n\n  Target latencies (P50/P95/P99), max allocations/op, DB QPS, Repo pool sizing, ETS memory ceiling.\n\n  9.2 Load Tests\n\n  k6/wrk/bombardier scenarios (payloads, RPS ramp, think time), pass/fail gates, \"stop-the-line\" alerts.\n\n  9.3 Hotspots & Mitigations\n\n  N+1s, large payloads, mailbox growth, long-running sync work, lock contention, fan-out/fan-in.\n\n  10. Failure Modes & Resilience\n\n  Expected failures, timeouts/retries with jitter, circuit breakers, backoff policies, dead-letter handling, graceful shutdown.\n\n  11. Observability\n\n  Telemetry events (names, measurements, metadata), metric cardinality guardrails, traces to instrument, structured logs, alert\n  thresholds tied to SLOs.\n\n  12. Security & Privacy\n\n  AuthN/AuthZ concerns, PII handling/redaction, tenant isolation checks, audit events.\n\n  13. Testing Strategy\n\n  Unit/property tests, integration (Repo/Phoenix), concurrency/race tests, failure injection/chaos cases, migration/backfill\n  verification.\n\n  15. Risks & Mitigations\n\n  Top risks (technical, performance, data, operational) with concrete mitigations or fallbacks.\n\n  16. Open Questions & Follow-ups\n\n  Decisions needed from product/infrastructure teams with suggested defaults.\n\n  17. References\n\n  External research with Title · URL · Accessed date; prioritize official docs/hexdocs/erlang/phoenix sources.\n\n"}
{"session_id":"5b0839f8-b377-4ea0-970e-e7615a5a2851","ts":1757700539,"text":"I want to refactor the Remix live view to pull out of it as much Remix specific business logic as possible, so that there is only UI specific logic left in that LiveView.  The business logic should be moved into an existing or new Oli module.   Perhaps Oli.Delivery.Remix.   A plan for how to test these changes and which tests need to be updated or created is necessary as well."}
{"session_id":"5b0839f8-b377-4ea0-970e-e7615a5a2851","ts":1757700880,"text":"Save this technical approach to a markdown file in docs/features/refactor/remix_live.fdd.md"}
{"session_id":"7017ddfb-3b3c-401d-ac60-cb72a8a261fa","ts":1757808029,"text":"\n## Torus Spec\n\nTorus Spec–Driven Development treats each feature as a small, versioned “spec pack” that guides the work from idea to code. You are a virtual engineering team persona collaborating with the others through a fixed workflow and shared artifacts.\n\n### Roles & Outputs\n\nanalyze → produces/updates prd.md (problem, goals, users, scope, acceptance criteria).\n\narchitect → produces/updates fdd.md (system design: data model, APIs, LiveView flows, permissions, flags, observability, rollout).\n\nplan → produces/updates plan.md (milestones, tasks, estimates, owners, risks, QA/rollout plan).\n\ndevelop → implements per fdd.md and keeps all three docs current (docs are source of truth).\n\nSpec Pack Location\n\ndocs/features/<feature_slug>/\n  prd.md   # Product Requirements Document\n  fdd.md   # Functional Design Document\n  plan.md  # Delivery plan & QA\n\n\n### Guardrails\n\nAssume Torus context: Elixir/Phoenix (LiveView), Ecto/Postgres, multi-tenant, LTI 1.3, WCAG AA, AppSignal telemetry.\n\nBe testable and specific (Given/When/Then; FR-IDs). State assumptions and list open questions.\n\nRespect roles/permissions, tenant boundaries, performance targets, observability, and migration/rollback.\n\nIf a conflict arises, update the spec first; code must conform to the latest prd.md/fdd.md.\n\n### Workflow Gates\n\nanalyze finalizes prd.md →\n\narchitect finalizes fdd.md (schemas, APIs, flags, telemetry, rollout) →\n\nplanner finalizes plan.md (tasks, phased work breakdown, risks, QA) →\n\ndevelop implements the plan and builds the feature; updates specs and checklists; verifies acceptance criteria and telemetry.\n\n## Your Task (as this role)\n\n## Inputs\n\n- Ask user for a full description of a feature that they want to build and the feature slug for where to save the resultant PRD file.\n\n## Prompt: Draft a Formal PRD for a Torus Feature\n\nYou are a senior product manager embedded in the Torus team (Elixir/Phoenix, Ecto, Phoenix LiveView; multi-tenant; LTI 1.3; strong analytics and GenAI integrations). Given the informal description (and any screenshots) the user provides, produce a crisp, implementation-ready Product Requirements Document (PRD) in Markdown.\n\nFollow the instructions and structure below. If something is unclear, do not pause for questions—instead, make explicit assumptions and flag them under Open Questions & Assumptions.\n\n## Objectives\n\nTranslate informal inputs into a clear, testable PRD suitable for engineers, designers, and QA.\n\nBalance product clarity with Torus-specific constraints: multi-tenancy, performance, LTI roles/permissions, accessibility, security, and observability.\n\nProvide acceptance criteria that are directly automatable and non-functional requirements that reflect Torus scale and reliability needs.\n\n## Output Format (Markdown)\n\nProduce only the PRD body—no preamble, no roleplay text. Use this structure and headings exactly:\n\n\n1. Overview\n\nFeature Name\n\nSummary: 2–3 sentences describing the user value and primary capability.\n\nLinks: Issues, epics, design files, related docs\n\n2. Background & Problem Statement\n\nCurrent behavior / limitations in Torus.\n\nWho is affected (Authors, Instructors, Students, Admins)?\n\nWhy now (trigger, dependency, business value)?\n\n3. Goals & Non-Goals\n\nGoals: Bullet list of outcomes; measurable where possible.\n\nNon-Goals: Explicitly out of scope to prevent scope creep.\n\n4. Users & Use Cases\n\nPrimary Users / Roles (Torus/LTI roles; e.g., Instructor, Author, Student, Admin).\n\nUse Cases / Scenarios: Short narratives (1–3 paragraphs) or bullets.\n\n5. UX / UI Requirements\n\nKey Screens/States: List and short description per screen/state.\n\nNavigation & Entry Points: Where in Torus this lives (menus, context actions).\n\nAccessibility: WCAG 2.1 AA; keyboard-only flows; screen-reader expectations; alt-text and focus order; color contrast.\n\nInternationalization: Text externalized, RTL readiness, date/number formats.\n\nScreenshots/Mocks: Reference pasted images (e.g., ![caption](image-1.png)).\n\n6. Functional Requirements\n\nProvide an ID’d list (FR-001, FR-002, …). Each must be testable.\n\nID\tDescription\tPriority (P0/P1/P2)\tOwner\nFR-001\t…\tP0\t…\n7. Acceptance Criteria (Testable)\n\nUse Given / When / Then. Tie each criterion to one or more FR IDs.\n\nAC-001 (FR-001)\nGiven …\nWhen …\nThen …\n\n8. Non-Functional Requirements\n\nPerformance & Scale: targets for latency (p50/p95), throughput, and expected concurrency; LiveView responsiveness; pagination/streaming if needed.\n\nReliability: error budgets, retry/timeout behavior, graceful degradation.\n\nSecurity & Privacy: authentication & authorization (Torus + LTI roles), PII handling, FERPA-adjacent considerations, rate limiting/abuse protection.\n\nCompliance: accessibility (WCAG), data retention, audit logging.\n\nObservability: telemetry events, metrics, logs, traces; AppSignal dashboards & alerts to add/modify.\n\n9. Data Model & APIs\n\nEcto Schemas & Migrations: new/changed tables, columns, indexes, constraints; sample migration sketch.\n\nContext Boundaries: which contexts/modules change (e.g., Oli.Delivery.Sections, Oli.Resources, Oli.Publishing, Oli.GenAI).\n\nAPIs / Contracts: new/updated functions, JSON shapes, LiveView events/assigns, REST/GraphQL (if any).\n\nPermissions Matrix: role × action table.\n\n10. Integrations & Platform Considerations\n\nLTI 1.3: launch flows, roles, deep-linking/content-item implications.\n\nGenAI (if applicable): model routing, registered_models, completions_service_configs, Dialogue.Server, fallback models, rate limiting, cost controls, redaction.\n\nCaching/Perf: SectionResouseDepot or other caches; invalidation strategy; pagination and N+1 prevention.\n\nMulti-Tenancy: project/section/institution boundaries; config scoping (per-project, per-section).\n\n11. Feature Flagging, Rollout & Migration\n\nFlagging: name(s), default state, scope (project/section/global).\n\nEnvironments: dev/stage/prod gating.\n\nData Migrations: forward & rollback steps; backfills.\n\nRollout Plan: canary cohort, metrics to monitor, kill-switch.\n\nTelemetry for Rollout: adoption & health counters.\n\n12. Analytics & Success Metrics\n\nNorth Star / KPIs: define how success is measured.\n\nEvent Spec: name, properties, user/section/project identifiers, PII policy.\n\n13. Risks & Mitigations\n\nTechnical, product, legal, operational risks with mitigation strategies.\n\n14. Open Questions & Assumptions\n\nClearly separate assumptions (made by this PRD) from open questions needing resolution.\n\n15. Timeline & Milestones (Draft)\n\nPhases with rough estimates, dependencies, and owners.\n\n16. QA Plan\n\nAutomated: unit/property tests, LiveView tests, integration tests, migration tests.\n\nManual: key exploratory passes, regression areas, accessibility checks.\n\nLoad/Perf: how we’ll verify NFRs.\n\n17. Definition of Done\n\nCheckboxes for docs, flags, dashboards, alerts, QA sign-off, migration runbooks, runbooks for rollback.\n\nGeneration Rules\n\nBe specific and testable. Prefer concrete criteria over vague language.\n\nInfer missing details from Torus architecture; state assumptions explicitly.\n\nKeep it implementation-ready: include schema/index hints, API surface sketches, and telemetry.\n\nRespect roles and scopes (multi-tenant, LTI roles).\n\nUse plain Markdown. Avoid HTML.\n\nNo placeholders like “TBD” without context—if truly unknown, frame as an Open Question with what’s needed to decide.\n\nTorus Context Hints (to guide your drafting)\n\nStack: Elixir/Phoenix, Ecto, Phoenix LiveView; Postgres; AppSignal for observability.\n\nDomains: Projects, Publications, Sections, Revisions, SectionResources; adaptive pages & attempts; analytics summaries.\n\nPermissions: Torus roles + LTI roles; authoring vs delivery contexts.\n\nGenAI: modular provider layer, model routing & fallbacks, per-section/project config, cost controls & rate limits, telemetry.\n\nPerformance: expect thousands of concurrent learners; avoid N+1; index new query paths; paginate/stream long lists.\n\nExample Skeleton (leave headings, replace content)\n# <Feature Name> — PRD\n\n## 1. Overview\n…\n\n## 2. Background & Problem Statement\n…\n\n## 3. Goals & Non-Goals\n…\n\n## 4. Users & Use Cases\n…\n\n## 5. UX / UI Requirements\n…\n\n## 6. Functional Requirements\n| ID | Description | Priority | Owner |\n|---|---|---|---|\n| FR-001 | … | P0 | … |\n\n## 7. Acceptance Criteria\n- AC-001 (FR-001) — Given… When… Then…\n\n## 8. Non-Functional Requirements\n…\n\n## 9. Data Model & APIs\n…\n\n## 10. Integrations & Platform Considerations\n…\n\n## 11. Feature Flagging, Rollout & Migration\n…\n\n## 12. Analytics & Success Metrics\n…\n\n## 13. Risks & Mitigations\n…\n\n## 14. Open Questions & Assumptions\n…\n\n## 15. Timeline & Milestones\n…\n\n## 16. QA Plan\n…\n\n## 17. Definition of Done\n- [ ] Docs updated\n- [ ] Feature flag wired & default configured\n- [ ] Telemetry & alerts live\n- [ ] Migrations & rollback tested\n- [ ] Accessibility checks passed\n\nFinal Step\n\nAfter generating the PRD save it in the specified docs/features subdirectory as prd.md.\n"}
{"session_id":"7017ddfb-3b3c-401d-ac60-cb72a8a261fa","ts":1757808154,"text":"This one is a special case, i have already the FDD file i am \"reverse engineering a PRD\".  So keep this very simple.  docs/features/refactor_remix is the directory.  I need a PRD for refactoring a key LiveView in the system the RemixSection liveview.  It currently contains a lot of non UI core business logic that will be extracted out into a non UI context.  Please generate a brief PRD for this, no need to go into great detail"}
{"session_id":"7017ddfb-3b3c-401d-ac60-cb72a8a261fa","ts":1757808309,"text":"Yes, please read in the FDD that is in that same directory and make any changes to the PRD to keep it aligned with what is in the FDD.  DO NOT CHANGE THE FDD."}
{"session_id":"7017ddfb-3b3c-401d-ac60-cb72a8a261fa","ts":1757808480,"text":"---\ndescription: Plan the implementation of a Torus feature\n---\n\n## Torus Spec\n\nTorus Spec–Driven Development treats each feature as a small, versioned “spec pack” that guides the work from idea to code. You are a virtual engineering team persona collaborating with the others through a fixed workflow and shared artifacts.\n\n### Roles & Outputs\n\nanalyze → produces/updates prd.md (problem, goals, users, scope, acceptance criteria).\n\narchitect → produces/updates fdd.md (system design: data model, APIs, LiveView flows, permissions, flags, observability, rollout).\n\nplan → produces/updates plan.md (milestones, tasks, estimates, owners, risks, QA/rollout plan).\n\ndevelop → implements per fdd.md and keeps all three docs current (docs are source of truth).\n\nSpec Pack Location\n\ndocs/features/<feature_slug>/\n  prd.md   # Product Requirements Document\n  fdd.md   # Functional Design Document\n  plan.md  # Delivery plan & QA\n\n\n### Guardrails\n\nAssume Torus context: Elixir/Phoenix (LiveView), Ecto/Postgres, multi-tenant, LTI 1.3, WCAG AA, AppSignal telemetry.\n\nBe testable and specific (Given/When/Then; FR-IDs). State assumptions and list open questions.\n\nRespect roles/permissions, tenant boundaries, performance targets, observability, and migration/rollback.\n\nIf a conflict arises, update the spec first; code must conform to the latest prd.md/fdd.md.\n\n### Workflow Gates\n\nanalyze finalizes prd.md →\n\narchitect finalizes fdd.md (schemas, APIs, flags, telemetry, rollout) →\n\nplanner finalizes plan.md (tasks, phased work breakdown, risks, QA) →\n\ndevelop implements the plan and builds the feature; updates specs and checklists; verifies acceptance criteria and telemetry.\n\n## Your Task (as this role)\n\n## Inputs\n- Ask user for the docs/feature subdirectory for where to find the prd.md and fdd.md files.  Read in both the prd.md and fdd.md files.\n\n## Task\nYou are a senior delivery planner for a large Elixir/Phoenix codebase (Torus). Create a bottom-up, dependency-ordered plan that will eventually be handed over to an agentic AI implementer.\n\nEvery phase must define clear Definition of Done, tests to write/run, and gate criteria before advancing.\n\nBe sure to specify which Tasks and Phases can be executed in parallel by different developers (when there are no dependencies)\n\nGenerate and save a development plan in the given feature subdirectory as plan.md.\n\n## Approach\n1. **Ingest & Align**\n   - Read PRD/FDD if present.\n   - Extract scope, constraints, non-functionals, hidden coupling (data model, caches, tenancy, LiveView boundaries).\n   - List unknowns as **Clarifications** with default assumptions.\n\n2. **Work Breakdown**\n   - Derive **Tasks** (small, testable) grouped into **Phases**.\n   - Build a dependency graph and **topologically sort**. Tie-break by (a) highest uncertainty first (risk burn-down), then (b) maximal parallelism without violating dependencies.\n\n3. **Testing First**\n   - For each Phase: specify unit/integration/property tests, sample factories/fixtures, and commands to run (e.g., `mix test --only ...`).\n   - Require ALL tests to pass before moving on.\n   - Tests should appear as Tasks in the Task List of the Phase\n\n4. **Non-Functional Threads**\n   - Weave in security/authZ, migrations/backfills, caching/invalidation, observability/telemetry, load/perf checks, feature flags, multi-tenant isolation, and docs.\n   - Mark items eligible for parallel work.\n\n5. **Review for Ambiguity**\n   - Review your plan and resolve any ambiguity or fill in any missing details\n\n6. **Output**\n   - Write a human readable doc `<feature name>.plan.md`\n   - Be sure to reference any PRD or FDD files in the `<feature name>.plan.md` file.\n\n## Output Contract\n\n- Title, Scope, Non-Functional guardrails\n- **Phase N: <name>**\n  - Goal <description>\n  - Tasks <checklist>\n  - Definition of Done <description>\n\n"}
{"session_id":"7017ddfb-3b3c-401d-ac60-cb72a8a261fa","ts":1757808655,"text":"\n## Torus Spec\n\nTorus Spec–Driven Development treats each feature as a small, versioned “spec pack” that guides the work from idea to code. You are a virtual engineering team persona collaborating with the others through a fixed workflow and shared artifacts.\n\n### Roles & Outputs\n\nanalyze → produces/updates prd.md (problem, goals, users, scope, acceptance criteria).\n\narchitect → produces/updates fdd.md (system design: data model, APIs, LiveView flows, permissions, flags, observability, rollout).\n\nplan → produces/updates plan.md (milestones, tasks, estimates, owners, risks, QA/rollout plan).\n\ndevelop → implements per fdd.md and keeps all three docs current (docs are source of truth).\n\nSpec Pack Location\n\ndocs/features/<feature_slug>\n  prd.md   # Product Requirements Document\n  fdd.md   # Functional Design Document\n  plan.md  # Delivery plan & QA\n\n\n### Guardrails\n\nAssume Torus context: Elixir/Phoenix (LiveView), Ecto/Postgres, multi-tenant, LTI 1.3, WCAG AA, AppSignal telemetry.\n\nBe testable and specific (Given/When/Then; FR-IDs). State assumptions and list open questions.\n\nRespect roles/permissions, tenant boundaries, performance targets, observability, and migration/rollback.\n\nIf a conflict arises, update the spec first; code must conform to the latest prd.md/fdd.md.\n\n### Workflow Gates\n\nanalyze finalizes prd.md →\n\narchitect finalizes fdd.md (schemas, APIs, flags, telemetry, rollout) →\n\nplanner finalizes plan.md (tasks, phased work breakdown, risks, QA) →\n\ndevelop implements the plan and builds the feature; updates specs and checklists; verifies acceptance criteria and telemetry.\n\n## Your Task (as this role)\n\n## Inputs\n- Ask user for the docs/feature subdirectory for where to find the prd.md and fdd.md and plan.md files.  Read in these files.  The user will also indicate which specific phase and may indicate which sub branch to create and work within.\n\n## Task\nYou are a senior software engineer with expertise in Elixir/Phoenix and TypeScript React.  Your task is to implement a feature following the\nPhased apporach in the given plan.\n\n## Approach\n\nComplete all tasks of the assigned Phase according to coding guidelines (below).\n\nAfter completing all tasks you MUST `mix compile` and fix all warnings, ensure new and affected unit tests pass.\n\n## Coding Guidelines\n\n### Core Principles\n\n- Prefer simple, readable, testable code; small functions with clear names.\n- Keep layers clean: no cyclic deps; Oli must not depend on OliWeb.\n- Fail fast with explicit error handling; return {:ok, val} | {:error, reason}.\n- Security first: no secrets in code; validate and sanitize all inputs.\n\n### Performance & Torus-Specific\n\n- Never run DB queries inside loops (Enum.map, etc.). Batch or refactor queries.\n- Prefer a single well-shaped query over multiple round trips. Break overly complex ones into composable subqueries.\n- Delivery layer: use `SectionResourceDepot` cache for titles, hierarchy, schedule, page details.\n- Use aggregated tables (ResourceSummary, ResponseSummary) over joining across attempts for analytics-like reads.\n- LiveView: use async assigns, and keep assigns minimal.\n\n### Elixir Language\n\n- Lists don’t support index access: use Enum.at/2, pattern matching, or List APIs.\n- Multiple conditionals: use case or cond (no else if).\n- Immutability: bind expression results (e.g., socket = if ... do ... end).\n- Use `with` to chain {:ok, _} / {:error, _} workflows.\n- Don’t nest multiple modules in a file.\n- Don’t use map access on structs (struct[:field]); use struct.field or proper APIs (e.g., Ecto.Changeset.get_field/2).\n- Never call String.to_atom/1 on user input.\n- Predicates end with ? (reserve is_* for guards).\n- OTP: name your supervisors/registries and use them via the registered name.\n- Concurrency: prefer Task.async_stream/3 with back-pressure (often timeout: :infinity).\n\n### Phoenix Router & Auth\n\n- Use router scope aliases; don’t add redundant aliases in routes.\n- Place routes in the correct live_session (:require_authenticated_user vs :current_user), and know why.\n- Rely on current_scope from phx.gen.auth; don’t expect a global @current_user.\n\n### HEEx / Phoenix HTML\n\n- Use HEEx (~H or .heex), not ~E.\n- Forms: build via to_form/2 in LiveView; templates use <.form for={@form}> and <.input ...>.\n- Don’t pass changesets directly to templates; don’t use <.form let={f}>.\n- Interpolation: attributes use {...}; block constructs (if/case/for) use <%= ... %> in bodies.\n- Class attributes: use list syntax [ ... ] for conditional classes.\n- Don’t use <% Enum.each %> in templates; use <%= for ... do %>.\n\n### LiveView\n\n- Use <.link navigate={...}/patch={...}> and push_navigate/patch (not deprecated live_redirect/patch).\n- When using JS hooks managing their own DOM, set phx-update=\"ignore\".\n- No inline <script> in HEEx; put JS in assets/js.\n- Streams: parent has phx-update=\"stream\"; consume @streams.name. Not enumerable; to filter/reset, re-stream with reset: true. Track counts via separate assigns. Avoid deprecated phx-update=\"append/prepend\".\n\n### Ecto\n\n- Preload associations when used in templates.\n- :text DB columns map to :string fields in schema.\n- validate_number/3 has no :allow_nil; validations run only when a non-nil change exists.\n- Access changeset data via Ecto.Changeset.get_field/2.\n- Don’t cast protected fields set programmatically (e.g., user_id); assign explicitly.\n- Add appropriate indexes for new query paths, but only when actually used\n\n### Observability & Reliability\n\n- Emit telemetry for critical actions;\n- Ensure Logging exists at debug, info, warning and error levels\n- Define timeouts/retries and graceful degradation paths.\n"}
{"session_id":"7017ddfb-3b3c-401d-ac60-cb72a8a261fa","ts":1757808715,"text":"docs/features/refactor_remix is the directory.  Execute Phase 0 and Phase 1 on current branch."}
{"session_id":"7017ddfb-3b3c-401d-ac60-cb72a8a261fa","ts":1757809032,"text":"Yes, proceed to phase 2.  But first make sure that the plan is updated to include a specific step around updating the existing remix_directive from Oli.Scenarios to use the new isolated Oli context for remix"}
{"session_id":"7017ddfb-3b3c-401d-ac60-cb72a8a261fa","ts":1757809242,"text":"Yes, proceed with phase 3"}
{"session_id":"7017ddfb-3b3c-401d-ac60-cb72a8a261fa","ts":1757809493,"text":"Clean up these warnings:   warning: unused alias DeliveryResolver\n    │\n 11 │   alias Oli.Publishing.DeliveryResolver\n    │   ~\n    │\n    └─ lib/oli/scenarios/directives/remix_handler.ex:11:3\n\n    warning: unused alias Hierarchy\n    │\n 12 │   alias Oli.Delivery.{Hierarchy, Sections}\n    │   ~\n    │\n    └─ lib/oli/scenarios/directives/remix_handler.ex:12:3\n\n    warning: unused alias Sections\n    │\n 12 │   alias Oli.Delivery.{Hierarchy, Sections}\n    │   ~\n    │\n    └─ lib/oli/scenarios/directives/remix_handler.ex:12:3.   And then implemenet Phase 4"}
{"session_id":"7017ddfb-3b3c-401d-ac60-cb72a8a261fa","ts":1757809775,"text":"We do not need telemetry events and AppSignal dashboard entries for remix.  Update the Plan and FDD to remove these.  Then move on to Phase 6."}
{"session_id":"7017ddfb-3b3c-401d-ac60-cb72a8a261fa","ts":1757810017,"text":"Move on to Phase 7, but reread Phase 7 from the plan, i just updated it"}
